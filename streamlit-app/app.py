# -*- coding: utf-8 -*-
"""
ã‚ªãƒªã‚³ãƒ³é¡§å®¢æº€è¶³åº¦Â®èª¿æŸ» TOPICSã‚µãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
Streamlitç‰ˆ - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯HANDOVER.mdã§ç®¡ç†
- ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ãƒ–ã«1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¿½åŠ ï¼ˆè©•ä¾¡é …ç›®åˆ¥ãƒ»éƒ¨é–€åˆ¥ã¨åŒæ§˜ï¼‰
- è©•ä¾¡é …ç›®åˆ¥ãƒ»éƒ¨é–€åˆ¥ã‚¿ãƒ–ã«1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¿½åŠ 
- å¹´åº¦æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£: æ›´æ–°æ—¥ã‚’å¹´åº¦åŸºæº–ã¨ã—ã¦ä½¿ç”¨ï¼ˆèª¿æŸ»æœŸé–“ã¯ä¸ä½¿ç”¨ï¼‰
- åŒç‡é †ä½å¯¾å¿œ: åŒç‚¹ã®å ´åˆã€ŒåŒç‡1ä½ã€ã¨ã—ã¦åˆ†æï¼ˆ2ç¤¾/3ç¤¾ä»¥ä¸Šå¯¾å¿œï¼‰
- é †ä½æŠ½å‡º: icon-rankã‚¯ãƒ©ã‚¹å„ªå…ˆã€è©•ä¾¡é …ç›®åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«é™¤å¤–
- å¹´åº¦åˆ—ã®èª¤æ¤œå‡ºã‚’é˜²æ­¢ï¼ˆå›ç­”è€…æ•°ï¼ˆæœ€æ–°å¹´ï¼‰ç­‰ã‚’é™¤å¤–ï¼‰
- å¹´åº¦å€¤ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå‹•çš„å¹´åº¦ç¯„å›²å¯¾å¿œï¼‰
- ã‚ªãƒªã‚³ãƒ³å†…éƒ¨Excelãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼è¡Œè‡ªå‹•æ¤œå‡ºï¼‰
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ”¹å–„: ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯æƒ…å ±ã®éå…¬é–‹åŒ–
- å‹•çš„å¹´åº¦æ¤œå‡º: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰å®Ÿéš›ã®ç™ºè¡¨å¹´åº¦ã‚’è‡ªå‹•åˆ¤å®š
"""

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
__version__ = "Î²ç‰ˆ"

import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®šï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’INFOã«å¤‰æ›´æ¨å¥¨ï¼‰
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

import os
import streamlit as st
import pandas as pd
from io import BytesIO
from datetime import datetime
from scraper import OriconScraper
from analyzer import TopicsAnalyzer, HistoricalAnalyzer, _year_sort_key

# ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ç”Ÿæˆãƒ»æ­£èª¤ãƒã‚§ãƒƒã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (v8.0è¿½åŠ )
try:
    from release_tab import render_release_tab, RELEASE_FEATURES_AVAILABLE
except ImportError as e:
    logger.warning(f"ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹æ©Ÿèƒ½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    RELEASE_FEATURES_AVAILABLE = False

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã®æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°ï¼ˆç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡ï¼‰
# Streamlit Cloud: Secrets ã§ ENABLE_UPLOAD_FEATURE = "true" ã‚’è¨­å®š
# ãƒ­ãƒ¼ã‚«ãƒ«: ç’°å¢ƒå¤‰æ•° ENABLE_UPLOAD_FEATURE=true ã‚’è¨­å®š
ENABLE_UPLOAD = os.environ.get("ENABLE_UPLOAD_FEATURE", "false").lower() == "true"


def create_excel_export(ranking_name, overall_data, item_data, dept_data, historical_data, used_urls=None):
    """å–å¾—ãƒ‡ãƒ¼ã‚¿ã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    output = BytesIO()

    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        workbook = writer.book

        # === ã‚·ãƒ¼ãƒˆ1: ã‚µãƒãƒªãƒ¼ ===
        summary_data = []
        records = historical_data.get("historical_records", {})
        summary = records.get("summary", {})

        if summary.get("max_consecutive"):
            mc = summary["max_consecutive"]
            summary_data.append(["æœ€é•·é€£ç¶š1ä½", mc["company"], f"{mc['years']}å¹´é€£ç¶š", f"{mc['start_year']}ã€œ{mc['end_year']}"])
        if summary.get("all_time_high"):
            ath = summary["all_time_high"]
            summary_data.append(["éå»æœ€é«˜å¾—ç‚¹", ath["company"], f"{ath['score']}ç‚¹", f"{ath['year']}å¹´"])
        if summary.get("most_wins"):
            mw = summary["most_wins"]
            summary_data.append(["æœ€å¤š1ä½ç²å¾—", mw["company"], f"{mw['wins']}å›", f"{mw['total_years']}å¹´ä¸­"])

        if summary_data:
            df_summary = pd.DataFrame(summary_data, columns=["è¨˜éŒ²", "ä¼æ¥­å", "æ•°å€¤", "è©³ç´°"])
            df_summary.to_excel(writer, sheet_name="ã‚µãƒãƒªãƒ¼", index=False)

        # === ã‚·ãƒ¼ãƒˆ2: ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå…¨å¹´åº¦ï¼‰ ===
        all_overall = []
        for year in sorted(overall_data.keys(), key=_year_sort_key, reverse=True):
            for item in overall_data[year]:
                all_overall.append({
                    "å¹´åº¦": year,
                    "é †ä½": item.get("rank"),
                    "ä¼æ¥­å": item.get("company"),
                    "å¾—ç‚¹": item.get("score")
                })
        if all_overall:
            pd.DataFrame(all_overall).to_excel(writer, sheet_name="ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°", index=False)

        # === ã‚·ãƒ¼ãƒˆ3: çµŒå¹´æ¯”è¼ƒï¼ˆãƒ”ãƒœãƒƒãƒˆï¼‰ ===
        companies = set()
        for year_data in overall_data.values():
            for item in year_data:
                companies.add(item.get("company", ""))

        pivot_data = []
        for company in sorted(companies):
            if not company:
                continue
            row = {"ä¼æ¥­å": company}
            for year in sorted(overall_data.keys(), key=_year_sort_key):
                score = None
                rank = None
                for item in overall_data.get(year, []):
                    if item.get("company") == company:
                        score = item.get("score")
                        rank = item.get("rank")
                        break
                row[f"{year}å¹´_å¾—ç‚¹"] = score if score is not None else ""
                row[f"{year}å¹´_é †ä½"] = rank if rank is not None else ""
            pivot_data.append(row)
        if pivot_data:
            pd.DataFrame(pivot_data).to_excel(writer, sheet_name="çµŒå¹´æ¯”è¼ƒ", index=False)

        # === ã‚·ãƒ¼ãƒˆ4: é€£ç¶š1ä½è¨˜éŒ² ===
        consecutive = records.get("consecutive_wins", [])
        if consecutive:
            df_cons = pd.DataFrame([
                {
                    "ä¼æ¥­å": r["company"],
                    "é€£ç¶šå¹´æ•°": r["years"],
                    "é–‹å§‹å¹´": r["start_year"],
                    "çµ‚äº†å¹´": r["end_year"],
                    "ç¶™ç¶šä¸­": "â—‹" if r.get("is_current") else ""
                }
                for r in consecutive
            ])
            df_cons.to_excel(writer, sheet_name="é€£ç¶š1ä½è¨˜éŒ²", index=False)

        # === ã‚·ãƒ¼ãƒˆ5: 1ä½ç²å¾—å›æ•° ===
        most_wins = records.get("most_wins", [])
        if most_wins:
            df_wins = pd.DataFrame([
                {
                    "ä¼æ¥­å": r["company"],
                    "1ä½å›æ•°": r["wins"],
                    "ç·å¹´æ•°": r["total_years"],
                    "ç²å¾—ç‡": f"{r['wins']/r['total_years']*100:.1f}%" if r['total_years'] > 0 else "0.0%",
                    "ç²å¾—å¹´": ", ".join(map(str, r["years"]))
                }
                for r in most_wins
            ])
            df_wins.to_excel(writer, sheet_name="1ä½ç²å¾—å›æ•°", index=False)

        # === ã‚·ãƒ¼ãƒˆ6: éå»æœ€é«˜å¾—ç‚¹ ===
        highest = records.get("highest_scores", [])
        if highest:
            df_high = pd.DataFrame([
                {
                    "é †ä½": i,
                    "ä¼æ¥­å": r["company"],
                    "å¾—ç‚¹": r["score"],
                    "å¹´åº¦": r["year"],
                    "ãã®å¹´ã®é †ä½": r["rank"]
                }
                for i, r in enumerate(highest[:20], 1)
            ])
            df_high.to_excel(writer, sheet_name="éå»æœ€é«˜å¾—ç‚¹", index=False)

        # === ã‚·ãƒ¼ãƒˆ7ã€œ: è©•ä¾¡é …ç›®åˆ¥ ===
        for item_name, year_data in item_data.items():
            if isinstance(year_data, dict):
                item_rows = []
                for year in sorted(year_data.keys(), key=_year_sort_key, reverse=True):
                    for item in year_data.get(year, []):
                        item_rows.append({
                            "å¹´åº¦": year,
                            "é †ä½": item.get("rank"),
                            "ä¼æ¥­å": item.get("company"),
                            "å¾—ç‚¹": item.get("score")
                        })
                if item_rows:
                    sheet_name = f"é …ç›®_{item_name[:20]}"
                    sheet_name = sheet_name.replace("/", "_").replace("\\", "_")[:31]
                    pd.DataFrame(item_rows).to_excel(writer, sheet_name=sheet_name, index=False)

        # === éƒ¨é–€åˆ¥ ===
        for dept_name, year_data in dept_data.items():
            if isinstance(year_data, dict):
                dept_rows = []
                for year in sorted(year_data.keys(), key=_year_sort_key, reverse=True):
                    for item in year_data.get(year, []):
                        dept_rows.append({
                            "å¹´åº¦": year,
                            "é †ä½": item.get("rank"),
                            "ä¼æ¥­å": item.get("company"),
                            "å¾—ç‚¹": item.get("score")
                        })
                if dept_rows:
                    sheet_name = f"éƒ¨é–€_{dept_name[:20]}"
                    sheet_name = sheet_name.replace("/", "_").replace("\\", "_")[:31]
                    pd.DataFrame(dept_rows).to_excel(writer, sheet_name=sheet_name, index=False)

        # === å‚è€ƒè³‡æ–™ï¼ˆURLï¼‰ã‚·ãƒ¼ãƒˆ ===
        if used_urls:
            url_rows = []
            for item in used_urls.get("overall", []):
                url_rows.append({
                    "ã‚«ãƒ†ã‚´ãƒª": "ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°",
                    "å¹´åº¦/é …ç›®": item.get("year", ""),
                    "URL": item.get("url", ""),
                    "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "æˆåŠŸ" if item.get("status") == "success" else "å¤±æ•—"
                })
            for item in used_urls.get("items", []):
                url_rows.append({
                    "ã‚«ãƒ†ã‚´ãƒª": "è©•ä¾¡é …ç›®åˆ¥",
                    "å¹´åº¦/é …ç›®": item.get("name", ""),
                    "URL": item.get("url", ""),
                    "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "æˆåŠŸ" if item.get("status") == "success" else "å¤±æ•—"
                })
            for item in used_urls.get("departments", []):
                url_rows.append({
                    "ã‚«ãƒ†ã‚´ãƒª": "éƒ¨é–€åˆ¥",
                    "å¹´åº¦/é …ç›®": item.get("name", ""),
                    "URL": item.get("url", ""),
                    "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "æˆåŠŸ" if item.get("status") == "success" else "å¤±æ•—"
                })
            if url_rows:
                pd.DataFrame(url_rows).to_excel(writer, sheet_name="å‚è€ƒè³‡æ–™URL", index=False)

    output.seek(0)
    return output.getvalue()


def parse_uploaded_excel(uploaded_file, specified_year=None):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

    å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
    1. æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå¹´åº¦åˆ—ã‚ã‚Šï¼‰
    2. ã‚ªãƒªã‚³ãƒ³å†…éƒ¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå¹´åº¦ãªã—ã€ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒ3è¡Œç›®ä»¥é™ï¼‰
    3. è©•ä¾¡é …ç›®ã‚·ãƒ¼ãƒˆï¼ˆ1åˆ—ç›®ã«è©•ä¾¡é …ç›®åï¼‰
    4. éƒ¨é–€åˆ¥ã‚·ãƒ¼ãƒˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ä¸Šã«ã‚«ãƒ†ã‚´ãƒªåï¼‰

    Args:
        uploaded_file: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        specified_year: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸå¹´åº¦ï¼ˆNoneã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬ï¼‰
    """
    try:
        xl = pd.ExcelFile(uploaded_file)
        sheet_names = xl.sheet_names

        overall_data = {}
        item_data = {}
        dept_data = {}

        # å¹´åº¦ã‚’æ±ºå®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®š > ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬ > ç¾åœ¨å¹´ï¼‰
        if specified_year:
            inferred_year = specified_year
        else:
            filename = uploaded_file.name if hasattr(uploaded_file, 'name') else ""
            import re
            year_match = re.search(r'20\d{2}', filename)
            if year_match:
                inferred_year = int(year_match.group())
            else:
                inferred_year = datetime.now().year

        for sheet_name in sheet_names:
            # ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‚·ãƒ¼ãƒˆ
            skip_sheets = ['ç¶™ç¶šåˆ©ç”¨æ„å‘', 'æ¨å¥¨æ„å‘', 'ä½œæ¥­ç”¨']
            if any(skip in sheet_name for skip in skip_sheets):
                continue

            # ã¾ãšãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã§èª­ã¿è¾¼ã‚“ã§ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡º
            df_raw = pd.read_excel(xl, sheet_name=sheet_name, header=None)

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡ºï¼ˆ"é †ä½"ã‚’å«ã¿ã€"ä¼æ¥­"ã¾ãŸã¯"ãƒ©ãƒ³ã‚­ãƒ³ã‚°"ã‚’å«ã‚€è¡Œï¼‰
            # IDåˆ—ã¯å¿…é ˆæ¡ä»¶ã‹ã‚‰é™¤å¤–ï¼ˆæ±ç”¨æ€§å‘ä¸Šï¼‰
            header_row = None
            category_name = None  # éƒ¨é–€åˆ¥ã‚·ãƒ¼ãƒˆã®ã‚«ãƒ†ã‚´ãƒªå
            for idx, row in df_raw.iterrows():
                row_str = ' '.join([str(v) for v in row.values if pd.notna(v)])
                if 'é †ä½' in row_str and ('ä¼æ¥­' in row_str or 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°' in row_str or 'ä¼šç¤¾' in row_str):
                    header_row = idx
                    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ä¸Šã«ã‚«ãƒ†ã‚´ãƒªåãŒã‚ã‚‹å ´åˆï¼ˆéƒ¨é–€åˆ¥ã‚·ãƒ¼ãƒˆï¼‰
                    # æ§‹é€ : Row0=FX, Row1=ã‚·ãƒ¼ãƒˆå, Row2=ã‚«ãƒ†ã‚´ãƒªå, Row3=nï¼, Row4=ãƒ˜ãƒƒãƒ€ãƒ¼
                    if idx >= 2:
                        # Row2ã‚’å„ªå…ˆçš„ã«ç¢ºèªï¼ˆé€šå¸¸ã‚«ãƒ†ã‚´ãƒªåãŒã‚ã‚‹å ´æ‰€ï¼‰
                        for cat_idx in [2, idx - 2, idx - 1]:
                            if cat_idx < 0 or cat_idx >= idx:
                                continue
                            cat_row = df_raw.iloc[cat_idx]
                            cat_val = cat_row.iloc[0] if pd.notna(cat_row.iloc[0]) else None
                            if cat_val:
                                cat_str = str(cat_val)
                                # é™¤å¤–æ¡ä»¶: nï¼ã€ã‚·ãƒ¼ãƒˆåã€FXã€nan
                                if (cat_str not in ['nan', 'NaN', sheet_name, 'FX', 'è©•ä¾¡é …ç›®']
                                    and 'nï¼' not in cat_str
                                    and 'n=' not in cat_str
                                    and cat_str != sheet_name.replace('åˆ¥', '')):
                                    category_name = cat_str
                                    break
                    break

            if header_row is None:
                continue

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æŒ‡å®šã—ã¦èª­ã¿è¾¼ã¿
            df = pd.read_excel(xl, sheet_name=sheet_name, header=header_row)

            # å¹´åº¦åˆ—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆèª¤æ¤œå‡ºã‚’é˜²ããŸã‚å³å¯†ã«ï¼‰
            year_col = None
            year_exclude_patterns = ['å›ç­”è€…æ•°', 'æœ€æ–°å¹´', 'å›ç­”', 'è€…æ•°', 'å‰å¹´', 'æ˜¨å¹´', 'ä»Šå¹´', 'æ¯å¹´']
            for col in df.columns:
                col_str = str(col)
                if any(pattern in col_str for pattern in year_exclude_patterns):
                    continue
                if col_str == 'å¹´åº¦' or 'å¹´åº¦' in col_str:
                    year_col = col
                    break
                elif col_str == 'å¹´':
                    year_col = col
                    break
                elif len(col_str) == 5 and col_str.endswith('å¹´') and col_str[:4].isdigit():
                    year_col = col
                    break

            # ä¼æ¥­ååˆ—ã‚’æ¢ã™
            company_col = None
            for col in df.columns:
                col_str = str(col)
                if 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ä¼æ¥­' in col_str or 'ä¼æ¥­å' in col_str:
                    company_col = col
                    break
            if company_col is None:
                for col in df.columns:
                    col_str = str(col)
                    if 'ä¼æ¥­' in col_str or 'ä¼šç¤¾' in col_str:
                        company_col = col
                        break

            # é †ä½åˆ—ã‚’æ¢ã™ï¼ˆ"é †ä½"ã¨ã„ã†åˆ—åã‚’å„ªå…ˆï¼‰
            rank_col = None
            for col in df.columns:
                col_str = str(col)
                if col_str == 'é †ä½':
                    rank_col = col
                    break
            if rank_col is None:
                for col in df.columns:
                    col_str = str(col)
                    if 'é †ä½' in col_str:
                        rank_col = col
                        break

            # å¾—ç‚¹åˆ—ã‚’æ¢ã™ï¼ˆå„ªå…ˆé †ä½: ã‚¹ã‚³ã‚¢ > åˆè¨ˆ > å¾—ç‚¹ï¼‰
            score_col = None
            for col in df.columns:
                col_str = str(col)
                if col_str == 'ã‚¹ã‚³ã‚¢' or 'ã‚¹ã‚³ã‚¢' in col_str:
                    score_col = col
                    break
            if score_col is None:
                for col in df.columns:
                    if str(col) == 'åˆè¨ˆ':
                        score_col = col
                        break
            if score_col is None:
                for col in df.columns:
                    col_str = str(col)
                    if 'å¾—ç‚¹' in col_str or 'ç‚¹æ•°' in col_str:
                        score_col = col
                        break

            # è©•ä¾¡é …ç›®åˆ—ã‚’æ¢ã™ï¼ˆ1åˆ—ç›®ãŒè©•ä¾¡é …ç›®åã®å ´åˆï¼‰
            eval_item_col = None
            first_col = df.columns[0] if len(df.columns) > 0 else None
            first_col_str = str(first_col) if first_col is not None else ""

            # è©•ä¾¡é …ç›®ã‚·ãƒ¼ãƒˆã®åˆ¤å®š: 1åˆ—ç›®ãŒé †ä½/IDã§ãªãã€è©•ä¾¡é …ç›®åã£ã½ã„å ´åˆ
            if first_col_str not in ['é †ä½', 'ID', 'å¹´åº¦', 'rank', ''] and first_col_str == 'è©•ä¾¡é …ç›®':
                eval_item_col = first_col
            elif 'è©•ä¾¡é …ç›®' in sheet_name and first_col_str not in ['é †ä½', 'ID', 'å¹´åº¦', 'rank', '']:
                eval_item_col = first_col

            if company_col and (rank_col or score_col):
                for _, row in df.iterrows():
                    # å¹´åº¦ã®å–å¾—ï¼ˆv7.9: "2024å¹´"å½¢å¼ã«å¯¾å¿œï¼‰
                    if year_col and pd.notna(row.get(year_col)):
                        try:
                            year_str = str(row[year_col]).replace('å¹´', '').strip()
                            year = int(year_str)
                            # å‹•çš„å¹´åº¦ç¯„å›²: 2000å¹´ã‹ã‚‰ç¾åœ¨å¹´+5å¹´ã¾ã§
                            current_year = datetime.now().year
                            max_year = current_year + 5
                            if year < 2000 or year > max_year:
                                year = inferred_year
                        except (ValueError, TypeError):
                            year = inferred_year
                    else:
                        year = inferred_year

                    # ä¼æ¥­åã®å–å¾—
                    company = str(row[company_col]) if pd.notna(row.get(company_col)) else ""
                    if not company or company.lower() in ['nan', 'none', '']:
                        continue

                    # é †ä½ã®å–å¾—
                    try:
                        rank_val = row.get(rank_col) if rank_col else None
                        rank = int(rank_val) if rank_val is not None and pd.notna(rank_val) else None
                    except (ValueError, TypeError):
                        rank = None

                    # å¾—ç‚¹ã®å–å¾—
                    try:
                        score_val = row.get(score_col) if score_col else None
                        score = float(score_val) if score_val is not None and pd.notna(score_val) else None
                    except (ValueError, TypeError):
                        score = None

                    # è©•ä¾¡é …ç›®åã®å–å¾—
                    eval_item_name = None
                    if eval_item_col:
                        try:
                            val = row.get(eval_item_col)
                            eval_item_name = str(val) if pd.notna(val) and str(val) not in ['nan', 'None', 'è©•ä¾¡é …ç›®'] else None
                        except:
                            eval_item_name = None

                    # ã‚·ãƒ¼ãƒˆç¨®åˆ¥ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´
                    # 1. ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ç³»
                    if 'ç·åˆ' in sheet_name or 'å¯¾è±¡ä¼æ¥­' in sheet_name:
                        if year not in overall_data:
                            overall_data[year] = []
                        overall_data[year].append({
                            "rank": rank,
                            "company": company,
                            "score": score
                        })

                    # 2. è©•ä¾¡é …ç›®ã‚·ãƒ¼ãƒˆï¼ˆ1åˆ—ç›®ã«é …ç›®åãŒã‚ã‚‹ï¼‰
                    elif eval_item_name and ('è©•ä¾¡é …ç›®' in sheet_name or eval_item_col):
                        if eval_item_name not in item_data:
                            item_data[eval_item_name] = {}
                        if year not in item_data[eval_item_name]:
                            item_data[eval_item_name][year] = []
                        item_data[eval_item_name][year].append({
                            "rank": rank,
                            "company": company,
                            "score": score
                        })

                    # 3. éƒ¨é–€åˆ¥ã‚·ãƒ¼ãƒˆï¼ˆæ¥­æ…‹åˆ¥ã€æŠ•è³‡ã‚¹ã‚¿ã‚¤ãƒ«åˆ¥ã€åˆ©ç”¨ãƒãƒ£ãƒ¼ãƒˆåˆ¥ã€ãƒ¬ãƒ™ãƒ«åˆ¥ã€ã‚µãƒãƒ¼ãƒˆåˆ¥ã€éƒ¨é–€_XXXï¼‰
                    # v7.10: 'éƒ¨é–€'ã‚’è¿½åŠ ï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸExcelã®"éƒ¨é–€_XXX"ã‚·ãƒ¼ãƒˆã‚’èªè­˜ï¼‰
                    elif any(x in sheet_name for x in ['æ¥­æ…‹', 'æŠ•è³‡ã‚¹ã‚¿ã‚¤ãƒ«', 'åˆ©ç”¨ãƒãƒ£ãƒ¼ãƒˆ', 'ãƒãƒ£ãƒ¼ãƒˆ', 'ãƒ¬ãƒ™ãƒ«', 'ã‚µãƒãƒ¼ãƒˆ', 'åˆ¥', 'éƒ¨é–€']):
                        # ã‚«ãƒ†ã‚´ãƒªåãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ã‚·ãƒ¼ãƒˆåã‹ã‚‰æŠ½å‡º
                        # v7.10: "éƒ¨é–€_XXX"å½¢å¼ã«å¯¾å¿œï¼ˆä¾‹: "éƒ¨é–€_ç”·æ€§" â†’ "ç”·æ€§"ï¼‰
                        if category_name:
                            dept_name = category_name
                        elif sheet_name.startswith('éƒ¨é–€_'):
                            dept_name = sheet_name[3:]  # "éƒ¨é–€_" ã‚’é™¤å»
                        else:
                            dept_name = sheet_name.replace('åˆ¥', '')
                        if dept_name not in dept_data:
                            dept_data[dept_name] = {}
                        if year not in dept_data[dept_name]:
                            dept_data[dept_name][year] = []
                        dept_data[dept_name][year].append({
                            "rank": rank,
                            "company": company,
                            "score": score
                        })

        return overall_data, item_data, dept_data, None
    except Exception as e:
        import traceback
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–: ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯è©³ç´°ã¯ãƒ­ã‚°ã®ã¿ã«å‡ºåŠ›ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯éè¡¨ç¤ºï¼‰
        logger.error(f"Excelè§£æã‚¨ãƒ©ãƒ¼: {str(e)}\n{traceback.format_exc()}")
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿è¡¨ç¤º
        return None, None, None, f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


def merge_data(uploaded_data, scraped_data):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿å„ªå…ˆï¼‰"""
    merged = {}

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã«
    for year, data in scraped_data.items():
        merged[year] = data

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ãï¼ˆå„ªå…ˆï¼‰
    for year, data in uploaded_data.items():
        merged[year] = data

    return merged


def merge_nested_data(uploaded_data, scraped_data):
    """è©•ä¾¡é …ç›®åˆ¥ãƒ»éƒ¨é–€åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
    merged = {}

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã«
    for key, year_data in scraped_data.items():
        if key not in merged:
            merged[key] = {}
        if isinstance(year_data, dict):
            for year, data in year_data.items():
                merged[key][year] = data

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ãï¼ˆå„ªå…ˆï¼‰
    for key, year_data in uploaded_data.items():
        if key not in merged:
            merged[key] = {}
        if isinstance(year_data, dict):
            for year, data in year_data.items():
                merged[key][year] = data

    return merged


def detect_name_changes(used_urls, category="items"):
    """
    åŒã˜slugï¼ˆitem_slug/dept_pathï¼‰ã§ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ãŒç•°ãªã‚‹ã‚‚ã®ã‚’æ¤œå‡ºã—ã€åç§°å¤‰æ›´å±¥æ­´ã‚’è¿”ã™

    Args:
        used_urls: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‹ã‚‰å–å¾—ã—ãŸURLæƒ…å ±ï¼ˆpage_title, item_slug/dept_path, year ã‚’å«ã‚€ï¼‰
        category: "items"ï¼ˆè©•ä¾¡é …ç›®ï¼‰ã¾ãŸã¯ "departments"ï¼ˆéƒ¨é–€ï¼‰

    Returns:
        dict: {
            ç¾åœ¨ã®åç§°ï¼ˆãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆï¼‰: {
                "changes": [{from_name, to_name, change_year}, ...],
                "latest_name": ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å–å¾—ã—ãŸæœ€æ–°åç§°
            }
        }
    """
    if not used_urls:
        return {}

    url_items = used_urls.get(category, [])
    if not url_items:
        return {}

    # slugï¼ˆitem_slug ã¾ãŸã¯ dept_pathï¼‰ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    slug_key = "item_slug" if category == "items" else "dept_path"

    # slug â†’ [(page_title, year, link_name), ...] ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    slug_map = {}
    for item in url_items:
        status = item.get("status", "")
        if status != "success":
            continue

        slug = item.get(slug_key)
        page_title = item.get("page_title")
        year = item.get("year")
        link_name = item.get("name", "").replace(f"({year}å¹´)", "").strip() if year else item.get("name", "")

        if not slug or not year:
            continue

        if slug not in slug_map:
            slug_map[slug] = []
        slug_map[slug].append({
            "page_title": page_title,
            "year": year,
            "link_name": link_name
        })

    # åç§°å¤‰æ›´ã‚’æ¤œå‡º
    name_changes = {}
    for slug, items in slug_map.items():
        # å¹´åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
        items_sorted = sorted(items, key=lambda x: _year_sort_key(x["year"]))

        if len(items_sorted) < 2:
            continue

        # page_titleã®å¤‰åŒ–ã‚’è¿½è·¡ï¼ˆNoneã¯é™¤å¤–ï¼‰
        unique_titles = []
        for item in items_sorted:
            title = item["page_title"]
            year = item["year"]
            if title is None:
                continue
            if not unique_titles or unique_titles[-1][0] != title:
                unique_titles.append((title, year))

        # æœ€æ–°ã®ãƒªãƒ³ã‚¯åç§°ã‚’ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
        latest_link_name = items_sorted[-1]["link_name"]
        latest_page_title = None
        for item in reversed(items_sorted):
            if item["page_title"]:
                latest_page_title = item["page_title"]
                break

        if len(unique_titles) > 1:
            # åç§°å¤‰æ›´ãŒã‚ã£ãŸå ´åˆ
            changes = []
            for i, (title, year) in enumerate(unique_titles[:-1]):
                next_title = unique_titles[i + 1][0]
                next_year = unique_titles[i + 1][1]
                changes.append({
                    "from_name": title,
                    "to_name": next_title,
                    "change_year": next_year
                })
            name_changes[latest_link_name] = {
                "changes": changes,
                "latest_name": latest_page_title
            }
        elif latest_page_title and latest_page_title != latest_link_name:
            # åç§°å¤‰æ›´ã¯ãªã„ãŒã€æœ€æ–°åç§°ãŒãƒªãƒ³ã‚¯åã¨ç•°ãªã‚‹å ´åˆã‚‚è¨˜éŒ²
            name_changes[latest_link_name] = {
                "changes": [],
                "latest_name": latest_page_title
            }

    return name_changes


def display_historical_summary(records, prefix=""):
    """æ­´ä»£è¨˜éŒ²ãƒ»é€£ç¶šè¨˜éŒ²ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    if not records:
        return

    summary = records.get("summary", {})
    if summary:
        col1, col2, col3 = st.columns(3)
        with col1:
            if summary.get("max_consecutive"):
                mc = summary["max_consecutive"]
                st.metric(
                    f"{prefix}ğŸ¥‡ æœ€é•·é€£ç¶š1ä½",
                    f"{mc['company']}",
                    f"{mc['years']}å¹´é€£ç¶š ({mc['start_year']}ã€œ{mc['end_year']})"
                )
        with col2:
            if summary.get("all_time_high"):
                ath = summary["all_time_high"]
                st.metric(
                    f"{prefix}ğŸ“ˆ éå»æœ€é«˜å¾—ç‚¹",
                    f"{ath['score']}ç‚¹",
                    f"{ath['company']} ({ath['year']}å¹´)"
                )
        with col3:
            if summary.get("most_wins"):
                mw = summary["most_wins"]
                st.metric(
                    f"{prefix}ğŸ† æœ€å¤š1ä½ç²å¾—",
                    f"{mw['company']}",
                    f"{mw['wins']}å› / {mw['total_years']}å¹´ä¸­"
                )


def display_consecutive_wins_compact(records):
    """é€£ç¶š1ä½è¨˜éŒ²ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«è¡¨ç¤º"""
    consecutive = records.get("consecutive_wins", [])
    # è¤‡æ•°å›1ä½ã‚’ç²å¾—ã—ãŸã‚‚ã®ã®ã¿å¯¾è±¡ï¼ˆ1å¹´ã ã‘ã®å—è³ã¯é™¤å¤–ï¼‰
    consecutive_filtered = [r for r in consecutive if r.get("years", 0) >= 2]
    if consecutive_filtered:
        st.markdown("**ğŸ¥‡ é€£ç¶š1ä½è¨˜éŒ²ï¼ˆä¸Šä½10ä»¶ï¼‰**")
        cons_df = pd.DataFrame([
            {
                "ä¼æ¥­å": r["company"],
                "é€£ç¶šå¹´æ•°": f"{r['years']}å¹´",
                "æœŸé–“": f"{r['start_year']}ã€œ{r['end_year']}",
                "ç¶™ç¶šä¸­": "âœ…" if r.get("is_current") else ""
            }
            for r in consecutive_filtered[:10]
        ])
        st.dataframe(cons_df, use_container_width=True, hide_index=True)


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã‚ªãƒªã‚³ãƒ³é¡§å®¢æº€è¶³åº¦Â®èª¿æŸ» TOPICSã‚µãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“°",
    layout="wide"
)

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ğŸ“° ã‚ªãƒªã‚³ãƒ³é¡§å®¢æº€è¶³åº¦Â®èª¿æŸ» TOPICSã‚µãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("ã‚ªãƒªã‚³ãƒ³é¡§å®¢æº€è¶³åº¦èª¿æŸ»ã®çµŒå¹´çµæœã‚’èª¿æŸ»ã€‚é€£ç¶šè¨˜éŒ²ã‚„1ä½ç²å¾—å›æ•°ã®å‚ç…§ã«æ´»ç”¨ã„ãŸã ã‘ã¾ã™ã€‚")
st.warning("âš ï¸ **æ³¨æ„äº‹é …**: æƒ…å ±ã®æ­£ç¢ºæ€§ã¯æ‹…å½“è€…ãŒå¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
st.sidebar.header("âš™ï¸ è¨­å®š")

# ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¸æŠï¼ˆç´„200ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œï¼‰
ranking_options = {
    # ========================================
    # ä¿é™º
    # ========================================
    "è‡ªå‹•è»Šä¿é™ºï¼ˆãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå‹ï¼‰": "_insurance",
    "è‡ªå‹•è»Šä¿é™ºï¼ˆä»£ç†åº—å‹ï¼‰": "_insurance@type02",
    "è‡ªå‹•è»Šä¿é™ºï¼ˆFPæ¨å¥¨ï¼‰": "_insurance@type03",
    "ãƒã‚¤ã‚¯ä¿é™ºï¼ˆãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå‹ï¼‰": "_bike",
    "ãƒã‚¤ã‚¯ä¿é™ºï¼ˆä»£ç†åº—å‹ï¼‰": "_bike@type02",
    "è‡ªè»¢è»Šä¿é™º": "bicycle-insurance",
    "ç«ç½ä¿é™º": "fire-insurance",
    "æµ·å¤–æ—…è¡Œä¿é™º": "travel-insurance",
    "ãƒšãƒƒãƒˆä¿é™º": "_pet",
    "ç”Ÿå‘½ä¿é™º": "life-insurance",
    "åŒ»ç™‚ä¿é™º": "medical_insurance",
    "ãŒã‚“ä¿é™º": "cancer-insurance",
    "å­¦è³‡ä¿é™º": "educational-insurance",
    "ä¿é™ºç›¸è«‡ã‚·ãƒ§ãƒƒãƒ—": "_hokenshop",
    # ========================================
    # é‡‘èãƒ»æŠ•è³‡
    # ========================================
    "ãƒãƒƒãƒˆè¨¼åˆ¸ï¼ˆé¡§å®¢æº€è¶³åº¦ï¼‰": "_certificate",
    "ãƒãƒƒãƒˆè¨¼åˆ¸ï¼ˆFPè©•ä¾¡ï¼‰": "_certificate@type02",
    "NISAï¼ˆè¨¼åˆ¸ä¼šç¤¾ï¼‰": "_nisa",
    "NISAï¼ˆéŠ€è¡Œï¼‰": "_nisa@type02",
    "NISAï¼ˆFPè©•ä¾¡ï¼‰": "_nisa@type03",
    "iDeCoï¼ˆè¨¼åˆ¸ä¼šç¤¾ï¼‰": "ideco",
    "iDeCoï¼ˆFPè©•ä¾¡ï¼‰": "ideco@type02",
    "ãƒãƒƒãƒˆéŠ€è¡Œ": "_netbank/bank",
    "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒãƒ³ã‚­ãƒ³ã‚°": "_netbank/banking",
    "ä½å®…ãƒ­ãƒ¼ãƒ³": "_housingloan",
    "ä½å®…ãƒ­ãƒ¼ãƒ³ï¼ˆFPè©•ä¾¡ï¼‰": "_housingloan@type02",
    "å¤–è²¨é é‡‘": "foreign-currency-deposits",
    "FXï¼ˆé¡§å®¢æº€è¶³åº¦ï¼‰": "_fx",
    "FXï¼ˆFPè©•ä¾¡ï¼‰": "_fx@type02",
    "ã‚«ãƒ¼ãƒ‰ãƒ­ãƒ¼ãƒ³ï¼ˆéŠ€è¡Œç³»ï¼‰": "card-loan",
    "ã‚«ãƒ¼ãƒ‰ãƒ­ãƒ¼ãƒ³ï¼ˆãƒãƒ³ãƒãƒ³ã‚¯ï¼‰": "card-loan/nonbank",
    "ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬ï¼‰": "credit-card/general",
    "ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ï¼ˆå¹´ä¼šè²»ç„¡æ–™ï¼‰": "credit-card/free-annual",
    "ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ï¼ˆã‚´ãƒ¼ãƒ«ãƒ‰ï¼‰": "credit-card/gold-card",
    "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¹æ±ºæ¸ˆã‚¢ãƒ—ãƒª": "smartphone-payment",
    "æš—å·è³‡ç”£ï¼ˆç¾ç‰©å–å¼•ï¼‰": "cryptocurrency/cash-transaction",
    "æš—å·è³‡ç”£ï¼ˆè¨¼æ‹ é‡‘å–å¼•ï¼‰": "cryptocurrency/margin-transaction",
    "ãƒ­ãƒœã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "robo-advisor",
    # ========================================
    # ä½å®…ãƒ»ä¸å‹•ç”£
    # ========================================
    "ä¸å‹•ç”£ä»²ä»‹ å£²å´ï¼ˆãƒãƒ³ã‚·ãƒ§ãƒ³ï¼‰": "estate-agency-sell/mansion",
    "ä¸å‹•ç”£ä»²ä»‹ å£²å´ï¼ˆæˆ¸å»ºã¦ï¼‰": "estate-agency-sell/kodate",
    "ä¸å‹•ç”£ä»²ä»‹ å£²å´ï¼ˆåœŸåœ°ï¼‰": "estate-agency-sell/land",
    "ä¸å‹•ç”£ä»²ä»‹ è³¼å…¥ï¼ˆãƒãƒ³ã‚·ãƒ§ãƒ³ï¼‰": "estate-agency-buy/mansion",
    "ä¸å‹•ç”£ä»²ä»‹ è³¼å…¥ï¼ˆæˆ¸å»ºã¦ï¼‰": "estate-agency-buy/kodate",
    "åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ç®¡ç†ä¼šç¤¾ï¼ˆé¦–éƒ½åœï¼‰": "mansion-maintenance/syutoken",
    "åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ç®¡ç†ä¼šç¤¾ï¼ˆæ±æµ·ï¼‰": "mansion-maintenance/tokai",
    "åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ç®¡ç†ä¼šç¤¾ï¼ˆè¿‘ç•¿ï¼‰": "mansion-maintenance/kinki",
    "åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ç®¡ç†ä¼šç¤¾ï¼ˆä¹å·ï¼‰": "mansion-maintenance/kyusyu",
    "ãƒãƒ³ã‚·ãƒ§ãƒ³å¤§è¦æ¨¡ä¿®ç¹•": "mansion-large-repair",
    "ä¸å‹•ç”£ä»²ä»‹ è³ƒè²¸": "rental-housing",
    "è³ƒè²¸æƒ…å ±ã‚µã‚¤ãƒˆ": "rental-housing/website",
    "è³ƒè²¸ãƒãƒ³ã‚·ãƒ§ãƒ³": "rental-condominiums",
    "ãƒªãƒ•ã‚©ãƒ¼ãƒ ï¼ˆãƒ•ãƒ«ãƒªãƒ•ã‚©ãƒ¼ãƒ ï¼‰": "_reform/large",
    "ãƒªãƒ•ã‚©ãƒ¼ãƒ ï¼ˆæˆ¸å»ºã¦ï¼‰": "_reform/kodate",
    "ãƒªãƒ•ã‚©ãƒ¼ãƒ ï¼ˆãƒãƒ³ã‚·ãƒ§ãƒ³ï¼‰": "_reform/mansion",
    "æ–°ç¯‰åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ï¼ˆé¦–éƒ½åœï¼‰": "new-condominiums/syutoken",
    "æ–°ç¯‰åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ï¼ˆæ±æµ·ï¼‰": "new-condominiums/tokai",
    "æ–°ç¯‰åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ï¼ˆè¿‘ç•¿ï¼‰": "new-condominiums/kinki",
    "æ–°ç¯‰åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ï¼ˆä¹å·ï¼‰": "new-condominiums/kyusyu",
    "ãƒã‚¦ã‚¹ãƒ¡ãƒ¼ã‚«ãƒ¼ï¼ˆæ³¨æ–‡ä½å®…ï¼‰": "house-maker",
    "å»ºå£²ä½å®…PBï¼ˆæ±åŒ—ï¼‰": "new-ready-built-house/powerbuilder/tohoku",
    "å»ºå£²ä½å®…PBï¼ˆåŒ—é–¢æ±ï¼‰": "new-ready-built-house/powerbuilder/kitakanto",
    "å»ºå£²ä½å®…PBï¼ˆé¦–éƒ½åœï¼‰": "new-ready-built-house/powerbuilder/syutoken",
    "å»ºå£²ä½å®…PBï¼ˆæ±æµ·ï¼‰": "new-ready-built-house/powerbuilder/tokai",
    "å»ºå£²ä½å®…PBï¼ˆè¿‘ç•¿ï¼‰": "new-ready-built-house/powerbuilder/kinki",
    "å»ºå£²ä½å®…PBï¼ˆä¹å·ï¼‰": "new-ready-built-house/powerbuilder/kyusyu",
    # ========================================
    # ç”Ÿæ´»ã‚µãƒ¼ãƒ“ã‚¹
    # ========================================
    "ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ã‚µãƒ¼ãƒãƒ¼": "_waterserver",
    "ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ã‚µãƒ¼ãƒãƒ¼ï¼ˆæµ„æ°´å‹ï¼‰": "_waterserver/purifier",
    "å®¶äº‹ä»£è¡Œã‚µãƒ¼ãƒ“ã‚¹": "housekeeping",
    "ãƒã‚¦ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°": "house-cleaning",
    "ã‚³ã‚¤ãƒ³ãƒ©ãƒ³ãƒ‰ãƒªãƒ¼": "laundromat",
    "é£Ÿæå®…é…ï¼ˆé¦–éƒ½åœï¼‰": "food-delivery/syutoken",
    "é£Ÿæå®…é…ï¼ˆæ±æµ·ï¼‰": "food-delivery/tokai",
    "é£Ÿæå®…é…ï¼ˆè¿‘ç•¿ï¼‰": "food-delivery/kinki",
    "ãƒŸãƒ¼ãƒ«ã‚­ãƒƒãƒˆï¼ˆé¦–éƒ½åœï¼‰": "food-delivery/meal-kit/syutoken",
    "ãƒŸãƒ¼ãƒ«ã‚­ãƒƒãƒˆï¼ˆæ±æµ·ï¼‰": "food-delivery/meal-kit/tokai",
    "ãƒŸãƒ¼ãƒ«ã‚­ãƒƒãƒˆï¼ˆè¿‘ç•¿ï¼‰": "food-delivery/meal-kit/kinki",
    "ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼": "net-super",
    "ãƒ•ãƒ¼ãƒ‰ãƒ‡ãƒªãƒãƒªãƒ¼ã‚µãƒ¼ãƒ“ã‚¹": "food-delivery-service",
    "ãµã‚‹ã•ã¨ç´ç¨ã‚µã‚¤ãƒˆ": "hometown-tax-website",
    "ãƒˆãƒ©ãƒ³ã‚¯ãƒ«ãƒ¼ãƒ ï¼ˆãƒ¬ãƒ³ã‚¿ãƒ«åç´ï¼‰": "trunk-room/rental-storage-space",
    "ãƒˆãƒ©ãƒ³ã‚¯ãƒ«ãƒ¼ãƒ ï¼ˆã‚³ãƒ³ãƒ†ãƒŠï¼‰": "trunk-room/container",
    "ãƒˆãƒ©ãƒ³ã‚¯ãƒ«ãƒ¼ãƒ ï¼ˆå®…é…å‹ï¼‰": "trunk-room/delivery",
    "å¼•è¶Šã—ä¼šç¤¾": "_move",
    "ã‚«ãƒ¼ã‚·ã‚§ã‚¢ãƒªãƒ³ã‚°": "carsharing",
    "ãƒ¬ãƒ³ã‚¿ã‚«ãƒ¼": "rent-a-car",
    "æ ¼å®‰ãƒ¬ãƒ³ã‚¿ã‚«ãƒ¼": "rent-a-car/reasonable",
    "è»Šè²·å–ä¼šç¤¾": "_carbuyer",
    "ä¸­å¤è»Šæƒ…å ±ã‚µã‚¤ãƒˆ": "used-car-sell",
    "ãƒã‚¤ã‚¯è²©å£²åº—": "bike-sell",
    "è»Šæ¤œ": "vehicle-inspection",
    "ã‚«ãƒ¼ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚µãƒ¼ãƒ“ã‚¹": "car-maintenance",
    "ã‚«ãƒ•ã‚§": "cafe",
    "å®šé¡åˆ¶å‹•ç”»é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹": "svod",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šæ˜ ç”»ï¼‰": "svod/genre",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šæ´‹ç”»ï¼‰": "svod/genre/foreign-film",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šå›½å†…ãƒ‰ãƒ©ãƒï¼‰": "svod/genre/japanese-drama",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šæµ·å¤–ãƒ‰ãƒ©ãƒï¼‰": "svod/genre/foreign-drama",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šéŸ“å›½ãƒ‰ãƒ©ãƒï¼‰": "svod/genre/korean-drama",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šã‚¢ãƒ‹ãƒ¡ï¼‰": "svod/genre/anime",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šãƒãƒ©ã‚¨ãƒ†ã‚£ï¼‰": "svod/genre/variety",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼ï¼‰": "svod/genre/documentary",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šã‚¹ãƒãƒ¼ãƒ„ï¼‰": "svod/genre/sports",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šã‚ªãƒªã‚¸ãƒŠãƒ«ï¼‰": "svod/genre/original",
    "å‹•ç”»é…ä¿¡ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼šã‚­ãƒƒã‚ºï¼‰": "svod/genre/kids",
    "å­ã©ã‚‚å†™çœŸã‚¹ã‚¿ã‚¸ã‚ª": "kids-photo-studio",
    "é›»å­æ›¸ç±ã‚µãƒ¼ãƒ“ã‚¹": "ebook",
    "é›»å­ã‚³ãƒŸãƒƒã‚¯ã‚µãƒ¼ãƒ“ã‚¹": "manga-apps",
    "ãƒãƒ³ã‚¬ã‚¢ãƒ—ãƒªï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ï¼‰": "manga-apps/original",
    "ãƒãƒ³ã‚¬ã‚¢ãƒ—ãƒªï¼ˆå‡ºç‰ˆç¤¾ï¼‰": "manga-apps/publisher",
    "ãƒ–ãƒ©ãƒ³ãƒ‰å“è²·å–ï¼ˆåº—èˆ—ï¼‰": "brand-sell",
    "é›»åŠ›ä¼šç¤¾ï¼ˆå°å£²ï¼‰": "electricity/retailing",
    "å­ã©ã‚‚è¦‹å®ˆã‚ŠGPS": "child-gps",
    # ========================================
    # é€šä¿¡
    # ========================================
    "æºå¸¯ã‚­ãƒ£ãƒªã‚¢": "mobile-carrier",
    "ã‚­ãƒ£ãƒªã‚¢æ ¼å®‰ãƒ–ãƒ©ãƒ³ãƒ‰": "mobile-carrier/reasonable",
    "æ ¼å®‰SIM": "mvno",
    "æ ¼å®‰SIMï¼ˆSIMã®ã¿ï¼‰": "mvno/sim",
    "æ ¼å®‰ã‚¹ãƒãƒ›": "mvno/sp",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€": "_internet",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆåŒ—æµ·é“ï¼‰": "_internet/hokkaido",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆæ±åŒ—ï¼‰": "_internet/tohoku",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆé–¢æ±ï¼‰": "_internet/kanto",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆç”²ä¿¡è¶Šãƒ»åŒ—é™¸ï¼‰": "_internet/koshinetsu-hokuriku",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆæ±æµ·ï¼‰": "_internet/tokai",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆè¿‘ç•¿ï¼‰": "_internet/kinki",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆä¸­å›½ï¼‰": "_internet/chugoku",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆå››å›½ï¼‰": "_internet/shikoku",
    "ãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆä¹å·ãƒ»æ²–ç¸„ï¼‰": "_internet/kyusyu-okinawa",
    # ========================================
    # æ•™è‚²ï¼ˆå¡¾ãƒ»å—é¨“ï¼‰â€»juken.oricon.co.jp
    # ========================================
    "å¤§å­¦å—é¨“ å¡¾ãƒ»äºˆå‚™æ ¡ï¼ˆé¦–éƒ½åœï¼‰": "_college/syutoken",
    "å¤§å­¦å—é¨“ å¡¾ãƒ»äºˆå‚™æ ¡ï¼ˆæ±æµ·ï¼‰": "_college/tokai",
    "å¤§å­¦å—é¨“ å¡¾ãƒ»äºˆå‚™æ ¡ï¼ˆè¿‘ç•¿ï¼‰": "_college/kinki",
    "å¤§å­¦å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆé¦–éƒ½åœï¼‰": "college-individual/syutoken",
    "å¤§å­¦å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆæ±æµ·ï¼‰": "college-individual/tokai",
    "å¤§å­¦å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆè¿‘ç•¿ï¼‰": "college-individual/kinki",
    "å¤§å­¦å—é¨“ é›£é–¢å¤§å­¦ç‰¹åŒ–å‹ï¼ˆé¦–éƒ½åœï¼‰": "_college/elite",
    "å¤§å­¦å—é¨“ æ˜ åƒæˆæ¥­": "college-video",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆåŒ—æµ·é“ï¼‰": "highschool/hokkaido",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆæ±åŒ—ï¼‰": "highschool/tohoku",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆåŒ—é–¢æ±ï¼‰": "highschool/kitakanto",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆé¦–éƒ½åœï¼‰": "highschool/syutoken",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆç”²ä¿¡è¶Šãƒ»åŒ—é™¸ï¼‰": "highschool/koshinetsu-hokuriku",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆæ±æµ·ï¼‰": "highschool/tokai",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆè¿‘ç•¿ï¼‰": "highschool/kinki",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆä¸­å›½ãƒ»å››å›½ï¼‰": "highschool/chugoku-shikoku",
    "é«˜æ ¡å—é¨“ å¡¾ï¼ˆä¹å·ãƒ»æ²–ç¸„ï¼‰": "highschool/kyusyu",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆåŒ—æµ·é“ï¼‰": "highschool-individual/hokkaido",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆæ±åŒ—ï¼‰": "highschool-individual/tohoku",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆåŒ—é–¢æ±ï¼‰": "highschool-individual/kitakanto",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆé¦–éƒ½åœï¼‰": "highschool-individual/syutoken",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆç”²ä¿¡è¶Šãƒ»åŒ—é™¸ï¼‰": "highschool-individual/koshinetsu-hokuriku",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆæ±æµ·ï¼‰": "highschool-individual/tokai",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆè¿‘ç•¿ï¼‰": "highschool-individual/kinki",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆä¸­å›½ãƒ»å››å›½ï¼‰": "highschool-individual/chugoku-shikoku",
    "é«˜æ ¡å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾ï¼ˆä¹å·ãƒ»æ²–ç¸„ï¼‰": "highschool-individual/kyusyu",
    "ä¸­å­¦å—é¨“ å¡¾ï¼ˆé¦–éƒ½åœï¼‰": "_junior/syutoken",
    "ä¸­å­¦å—é¨“ å¡¾ï¼ˆæ±æµ·ï¼‰": "_junior/tokai",
    "ä¸­å­¦å—é¨“ å¡¾ï¼ˆè¿‘ç•¿ï¼‰": "_junior/kinki",
    "ä¸­å­¦å—é¨“ å€‹åˆ¥æŒ‡å°å¡¾": "_junior/individual",
    "å…¬ç«‹ä¸­é«˜ä¸€è²«æ ¡å¯¾ç­– å¡¾ï¼ˆé¦–éƒ½åœï¼‰": "public-junior/syutoken",
    "å…¬ç«‹ä¸­é«˜ä¸€è²«æ ¡å¯¾ç­– å¡¾ï¼ˆæ±æµ·ï¼‰": "public-junior/tokai",
    "å…¬ç«‹ä¸­é«˜ä¸€è²«æ ¡å¯¾ç­– å¡¾ï¼ˆè¿‘ç•¿ï¼‰": "public-junior/kinki",
    # ========================================
    # æ•™è‚²ï¼ˆé€šä¿¡ãƒ»è‹±èªãƒ»è³‡æ ¼ï¼‰â€»juken.oricon.co.jp
    # ========================================
    "é€šä¿¡æ•™è‚²ï¼ˆé«˜æ ¡ç”Ÿï¼‰": "online-study/highschool",
    "é€šä¿¡æ•™è‚²ï¼ˆä¸­å­¦ç”Ÿï¼‰": "online-study/junior-hs",
    "é€šä¿¡æ•™è‚²ï¼ˆå°å­¦ç”Ÿï¼‰": "online-study/elementary",
    "å®¶åº­æ•™å¸«": "tutor",
    "è£œç¿’å¡¾": "supplementary-school",
    "å¹¼å…ãƒ»å°å­¦ç”Ÿ å­¦ç¿’æ•™å®¤": "kids-school/intellectual",
    "å­ã©ã‚‚è‹±èªæ•™å®¤ï¼ˆå¹¼å…ï¼‰": "kids-english/preschooler",
    "å­ã©ã‚‚è‹±èªæ•™å®¤ï¼ˆå°å­¦ç”Ÿï¼‰": "kids-english/grade-schooler",
    "è‹±ä¼šè©±ã‚¹ã‚¯ãƒ¼ãƒ«": "_english",
    "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è‹±ä¼šè©±": "online-english",
    "é€šä¿¡è¬›åº§ï¼ˆFPï¼‰": "cc/fp",
    "é€šä¿¡è¬›åº§ï¼ˆåŒ»ç™‚äº‹å‹™ï¼‰": "cc/mo",
    "é€šä¿¡è¬›åº§ï¼ˆå®…å»ºï¼‰": "cc/takken",
    "é€šä¿¡è¬›åº§ï¼ˆç°¿è¨˜ï¼‰": "cc/bookkeeping",
    "é€šä¿¡è¬›åº§ï¼ˆTOEICï¼‰": "cc/toeic",
    "é€šä¿¡è¬›åº§ï¼ˆç¤¾ä¼šä¿é™ºåŠ´å‹™å£«ï¼‰": "cc/labor-and-social-security",
    "é€šä¿¡è¬›åº§ï¼ˆã‚±ã‚¢ãƒãƒã‚¸ãƒ£ãƒ¼ï¼‰": "cc/care-manager",
    "é€šä¿¡è¬›åº§ï¼ˆå…¬å‹™å“¡ï¼‰": "cc/public-officer",
    "é€šä¿¡è¬›åº§ï¼ˆITãƒ‘ã‚¹ãƒãƒ¼ãƒˆï¼‰": "cc/it-certification",
    "è³‡æ ¼ã‚¹ã‚¯ãƒ¼ãƒ«ï¼ˆFPï¼‰": "license/fp",
    "è³‡æ ¼ã‚¹ã‚¯ãƒ¼ãƒ«ï¼ˆå®…å»ºï¼‰": "license/takken",
    "è³‡æ ¼ã‚¹ã‚¯ãƒ¼ãƒ«ï¼ˆç°¿è¨˜ï¼‰": "license/bookkeeping",
    "è³‡æ ¼ã‚¹ã‚¯ãƒ¼ãƒ«ï¼ˆç¤¾ä¼šä¿é™ºåŠ´å‹™å£«ï¼‰": "license/labor-and-social-security",
    # ========================================
    # ã‚¹ãƒãƒ¼ãƒ„ãƒ»ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹
    # ========================================
    "ã‚­ãƒƒã‚ºã‚¹ã‚¤ãƒŸãƒ³ã‚°ã‚¹ã‚¯ãƒ¼ãƒ«ï¼ˆå¹¼å…ï¼‰": "kids-swimming/preschooler",
    "ã‚­ãƒƒã‚ºã‚¹ã‚¤ãƒŸãƒ³ã‚°ã‚¹ã‚¯ãƒ¼ãƒ«ï¼ˆå°å­¦ç”Ÿï¼‰": "kids-swimming/grade-schooler",
    "ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚¯ãƒ©ãƒ–": "_fitness",
    "24æ™‚é–“ã‚¸ãƒ ": "_fitness/24hours",
    "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°": "_fitness/service",
    # ========================================
    # è»¢è·ãƒ»äººæ â€»career.oricon.co.jp
    # ========================================
    "å°±æ´»ã‚µã‚¤ãƒˆ": "new-graduates-hiring-website",
    "é€†æ±‚äººå‹å°±æ´»ã‚µãƒ¼ãƒ“ã‚¹": "reversed-job-offer",
    "ã‚¢ãƒ«ãƒã‚¤ãƒˆæƒ…å ±ã‚µã‚¤ãƒˆ": "arbeit",
    "è»¢è·ã‚µã‚¤ãƒˆ": "job-change",
    "è»¢è·ã‚µã‚¤ãƒˆï¼ˆå¥³æ€§ï¼‰": "job-change_woman",
    "è»¢è·ã‚¹ã‚«ã‚¦ãƒˆã‚µãƒ¼ãƒ“ã‚¹": "job-change_scout",
    "è»¢è·ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ": "_agent",
    "çœ‹è­·å¸«è»¢è·": "_agent_nurse",
    "ä»‹è­·è»¢è·": "_agent_nursing",
    "ãƒã‚¤ã‚¯ãƒ©ã‚¹ãƒ»ãƒŸãƒ‰ãƒ«ã‚¯ãƒ©ã‚¹è»¢è·": "_agent_hi-and-middle-class",
    "æ´¾é£ä¼šç¤¾": "_staffing",
    "å·¥å ´ãƒ»è£½é€ æ¥­æ´¾é£": "_staffing_manufacture",
    "æ´¾é£æ±‚äººã‚µã‚¤ãƒˆ": "temp-staff",
    "æ±‚äººæƒ…å ±ã‚µãƒ¼ãƒ“ã‚¹": "employment",
    # ========================================
    # ãƒˆãƒ©ãƒ™ãƒ«
    # ========================================
    "æ—…è¡Œäºˆç´„ã‚µã‚¤ãƒˆï¼ˆå›½å†…ï¼‰": "bargain-hotels-website",
    "æ—…è¡Œäºˆç´„ã‚µã‚¤ãƒˆï¼ˆæµ·å¤–ï¼‰": "bargain-airline-website",
    "ãƒ„ã‚¢ãƒ¼æ¯”è¼ƒã‚µã‚¤ãƒˆ": "tours-website",
    # ========================================
    # ç¾å®¹ãƒ»ã‚¦ã‚¨ãƒ‡ã‚£ãƒ³ã‚°
    # ========================================
    "ãƒ–ãƒ©ã‚¤ãƒ€ãƒ«ã‚¨ã‚¹ãƒ†": "_esthe/bridal",
    "ãƒ•ã‚§ã‚¤ã‚·ãƒ£ãƒ«ã‚¨ã‚¹ãƒ†": "_esthe/facial",
    "ç—©èº«ãƒ»ãƒœãƒ‡ã‚£ã‚¨ã‚¹ãƒ†": "_esthe/slim",
    "ã‚µãƒ­ãƒ³æ¤œç´¢äºˆç´„ã‚µã‚¤ãƒˆ": "salon-website",
    "ãƒã‚¦ã‚¹ã‚¦ã‚¨ãƒ‡ã‚£ãƒ³ã‚°": "wedding-produce",
    "çµå©šç›¸è«‡æ‰€": "_marriage",
    # ========================================
    # å°å£²ãƒ»ãƒ¬ã‚¸ãƒ£ãƒ¼
    # ========================================
    "å®¶é›»é‡è²©åº—": "electronics-retail-store",
    "ãƒ‰ãƒ©ãƒƒã‚°ã‚¹ãƒˆã‚¢": "drug-store",
    "æ˜ ç”»é¤¨": "movie-theater",
    "ã‚«ãƒ©ã‚ªã‚±ãƒœãƒƒã‚¯ã‚¹": "karaoke",
    "ãƒ†ãƒ¼ãƒãƒ‘ãƒ¼ã‚¯": "theme-park",
    # ========================================
    # ãã®ä»–
    # ========================================
    "ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›": "custom"
}

# æ¤œç´¢å‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¸æŠ
all_rankings = list(ranking_options.keys())

search_keyword = st.sidebar.text_input(
    "ğŸ” ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’æ¤œç´¢ãƒ»é¸æŠ",
    placeholder="ä¾‹ï¼šä¿é™ºã€è»¢è·ã€è‹±ä¼šè©±ã€å¡¾"
)

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & é¸æŠUI
if search_keyword:
    filtered_rankings = [r for r in all_rankings if search_keyword.lower() in r.lower()]

    if filtered_rankings:
        st.sidebar.caption(f"âœ… {len(filtered_rankings)}ä»¶ãƒ’ãƒƒãƒˆ")
        selected_ranking = st.sidebar.radio(
            "é¸æŠã—ã¦ãã ã•ã„",
            filtered_rankings,
            label_visibility="collapsed"
        )
    else:
        st.sidebar.warning(f"ã€Œ{search_keyword}ã€ã«ä¸€è‡´ã™ã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒã‚ã‚Šã¾ã›ã‚“")
        st.sidebar.caption("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰ãˆã¦å†æ¤œç´¢ã—ã¦ãã ã•ã„")
        selected_ranking = None
else:
    # æœªå…¥åŠ›æ™‚ã¯ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è¡¨ç¤º
    st.sidebar.caption(f"ğŸ“Š å…¨{len(all_rankings)}ä»¶ - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§çµã‚Šè¾¼ã‚ã¾ã™")

    category_options = {
        "ä¿é™º": [r for r in all_rankings if "ä¿é™º" in r],
        "é‡‘èãƒ»æŠ•è³‡": [r for r in all_rankings if any(k in r for k in ["è¨¼åˆ¸", "NISA", "iDeCo", "éŠ€è¡Œ", "ãƒ­ãƒ¼ãƒ³", "ã‚«ãƒ¼ãƒ‰", "FX", "æš—å·", "ãƒ­ãƒœ", "å¤–è²¨", "æ±ºæ¸ˆ"])],
        "ä½å®…ãƒ»ä¸å‹•ç”£": [r for r in all_rankings if any(k in r for k in ["ä¸å‹•ç”£", "ãƒãƒ³ã‚·ãƒ§ãƒ³", "ä½å®…", "ãƒªãƒ•ã‚©ãƒ¼ãƒ ", "ãƒã‚¦ã‚¹", "å»ºå£²"])],
        "ç”Ÿæ´»ã‚µãƒ¼ãƒ“ã‚¹": [r for r in all_rankings if any(k in r for k in ["ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼", "å®¶äº‹", "ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°", "é£Ÿæ", "ãƒŸãƒ¼ãƒ«", "ã‚¹ãƒ¼ãƒ‘ãƒ¼", "ãƒ‡ãƒªãƒãƒªãƒ¼", "ãµã‚‹ã•ã¨", "ãƒˆãƒ©ãƒ³ã‚¯", "å¼•è¶Š", "ã‚«ãƒ¼", "è»Š", "ãƒã‚¤ã‚¯", "ã‚«ãƒ•ã‚§", "å‹•ç”»", "å†™çœŸ", "é›»å­", "ãƒãƒ³ã‚¬", "ãƒ–ãƒ©ãƒ³ãƒ‰", "é›»åŠ›", "è¦‹å®ˆã‚Š", "ãƒ©ãƒ³ãƒ‰ãƒªãƒ¼"])],
        "é€šä¿¡": [r for r in all_rankings if any(k in r for k in ["æºå¸¯", "ã‚­ãƒ£ãƒªã‚¢", "SIM", "ã‚¹ãƒãƒ›", "ãƒ—ãƒ­ãƒã‚¤ãƒ€"])],
        "æ•™è‚²ãƒ»å¡¾": [r for r in all_rankings if any(k in r for k in ["å—é¨“", "å¡¾", "äºˆå‚™æ ¡", "æŒ‡å°", "é€šä¿¡æ•™è‚²", "å®¶åº­æ•™å¸«", "è£œç¿’", "å­¦ç¿’", "è‹±èª", "è‹±ä¼šè©±", "é€šä¿¡è¬›åº§", "è³‡æ ¼", "ã‚¹ã‚¤ãƒŸãƒ³ã‚°"])],
        "è»¢è·ãƒ»äººæ": [r for r in all_rankings if any(k in r for k in ["å°±æ´»", "æ±‚äºº", "ã‚¢ãƒ«ãƒã‚¤ãƒˆ", "è»¢è·", "æ´¾é£", "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"])],
        "ãƒˆãƒ©ãƒ™ãƒ«ãƒ»ç¾å®¹ãƒ»ãã®ä»–": [r for r in all_rankings if any(k in r for k in ["æ—…è¡Œ", "ãƒ„ã‚¢ãƒ¼", "ã‚¨ã‚¹ãƒ†", "ã‚µãƒ­ãƒ³", "ã‚¦ã‚¨ãƒ‡ã‚£ãƒ³ã‚°", "çµå©š", "å®¶é›»", "ãƒ‰ãƒ©ãƒƒã‚°", "æ˜ ç”»", "ã‚«ãƒ©ã‚ªã‚±", "ãƒ†ãƒ¼ãƒãƒ‘ãƒ¼ã‚¯", "ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹", "ã‚¸ãƒ ", "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«"])],
    }

    selected_category = st.sidebar.selectbox(
        "ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
        ["-- ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ --"] + list(category_options.keys()) + ["ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›"]
    )

    if selected_category == "-- ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ --":
        selected_ranking = None
        st.sidebar.info("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã™ã‚‹ã‹ã€ä¸Šã®æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã—ã¦ãã ã•ã„")
    elif selected_category == "ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›":
        selected_ranking = "ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›"
    else:
        category_rankings = category_options.get(selected_category, [])
        if category_rankings:
            selected_ranking = st.sidebar.radio(
                f"{selected_category}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
                category_rankings,
                label_visibility="collapsed"
            )
        else:
            selected_ranking = None

# ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¸æŠã®å‡¦ç†
if selected_ranking is None:
    ranking_slug = None
    ranking_name = None
elif selected_ranking == "ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›":
    ranking_slug = st.sidebar.text_input(
        "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®URLå",
        placeholder="ä¾‹: mobile-carrier"
    )
    ranking_name = st.sidebar.text_input(
        "ãƒ©ãƒ³ã‚­ãƒ³ã‚°åï¼ˆè¡¨ç¤ºç”¨ï¼‰",
        placeholder="ä¾‹: æºå¸¯ã‚­ãƒ£ãƒªã‚¢"
    )
else:
    ranking_slug = ranking_options[selected_ranking]
    ranking_name = selected_ranking

# å¹´åº¦é¸æŠ
# æ³¨æ„: current_yearã¯Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®æœ€æ–°å¹´åº¦ï¼ˆã‚ªãƒªã‚³ãƒ³ã‚µã‚¤ãƒˆã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æœ€æ–°ï¼‰
# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å¹´åº¦ã¯åˆ¥é€”æŒ‡å®šå¯èƒ½
# datetime.now().year ã‚’ä½¿ç”¨ã—ã¦å‹•çš„ã«ç¾åœ¨å¹´ã‚’å–å¾—
current_year = datetime.now().year  # Webã‚µã‚¤ãƒˆã®æœ€æ–°å¹´åº¦ï¼ˆå‹•çš„ï¼‰
start_year = 2006

year_option = st.sidebar.radio(
    "éå»ãƒ‡ãƒ¼ã‚¿å–å¾—ç¯„å›²",
    ["ç›´è¿‘3å¹´", "ç›´è¿‘5å¹´", "å…¨å¹´åº¦ï¼ˆ2006å¹´ã€œï¼‰", "ã‚«ã‚¹ã‚¿ãƒ ç¯„å›²"],
    index=2  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨å¹´åº¦
)

if year_option == "ç›´è¿‘3å¹´":
    year_range = (current_year - 2, current_year)
elif year_option == "ç›´è¿‘5å¹´":
    year_range = (current_year - 4, current_year)
elif year_option == "å…¨å¹´åº¦ï¼ˆ2006å¹´ã€œï¼‰":
    year_range = (start_year, current_year)
else:
    year_range = st.sidebar.slider(
        "å¹´åº¦ç¯„å›²ã‚’é¸æŠ",
        min_value=start_year,
        max_value=current_year,
        value=(current_year - 4, current_year)
    )

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'results_data' not in st.session_state:
    st.session_state.results_data = None

# å®Ÿè¡Œãƒœã‚¿ãƒ³ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿å–å¾—ç¯„å›²ã®ç›´ä¸‹ã«é…ç½®ï¼‰
run_button = st.sidebar.button("ğŸš€ TOPICSå‡ºã—å®Ÿè¡Œ", type="primary", use_container_width=True)

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰- ç’°å¢ƒå¤‰æ•°ã§æœ‰åŠ¹åŒ–æ™‚ã®ã¿è¡¨ç¤º
uploaded_file = None
upload_year = None

if ENABLE_UPLOAD:
    st.sidebar.markdown("---")
    with st.sidebar.expander("ğŸ“ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ»éæ¨å¥¨ï¼‰", expanded=False):
        st.caption("âš ï¸ é€šå¸¸ã¯Webã‹ã‚‰è‡ªå‹•å–å¾—ã•ã‚Œã‚‹ãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯ä¸è¦ã§ã™ã€‚æœªå…¬é–‹ã®æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹å ´åˆã®ã¿ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        uploaded_file = st.file_uploader(
            "æœ€æ–°ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°Excelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=["xlsx", "xls"],
            help="æœ€æ–°ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€éå»ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆã—ã¦åˆ†æã—ã¾ã™",
            key="excel_uploader"
        )

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å¹´åº¦æŒ‡å®š
        if uploaded_file:
            st.success(f"âœ… {uploaded_file.name}")
            upload_year = st.number_input(
                "ğŸ“… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å¹´åº¦",
                min_value=2006,
                max_value=2030,
                value=2026,
                help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å¹´åº¦ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: 2026å¹´ç™ºè¡¨ãƒ‡ãƒ¼ã‚¿ãªã‚‰2026ï¼‰"
            )
            st.info(f"ğŸ“Œ **{upload_year}å¹´**ã®ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã€ãã‚Œä»¥å¤–ã®å¹´åº¦ã¯Webã‹ã‚‰å–å¾—ã—ã¦çµ±åˆã—ã¾ã™")

# å®Ÿè¡Œãƒœã‚¿ãƒ³å‡¦ç†
if run_button:

    if not ranking_slug:
        st.error("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®URLåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        # å®Ÿè¡Œé–‹å§‹æ™‚ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆå‰å›çµæœãŒæ®‹ã‚‰ãªã„ã‚ˆã†ã«ï¼‰
        st.session_state.results_data = None

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        progress_bar = st.progress(0)
        status_text = st.empty()

        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤ºã‚¨ãƒªã‚¢
        debug_expander = st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°", expanded=False)
        debug_logs = []

        def log(message):
            debug_logs.append(message)
            with debug_expander:
                st.text("\n".join(debug_logs))

        try:
            uploaded_overall = {}
            uploaded_item = {}
            uploaded_dept = {}
            uploaded_years = set()

            # Step 1: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°è§£æ
            if uploaded_file:
                status_text.text("ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­...")
                progress_bar.progress(10)

                uploaded_overall, uploaded_item, uploaded_dept, error = parse_uploaded_excel(uploaded_file, upload_year)

                if error:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {error}")
                    st.stop()

                if uploaded_overall is None:
                    uploaded_overall = {}
                if uploaded_item is None:
                    uploaded_item = {}
                if uploaded_dept is None:
                    uploaded_dept = {}

                uploaded_years = set(uploaded_overall.keys())
                log(f"[OK] ãƒ•ã‚¡ã‚¤ãƒ«è§£æå®Œäº†: {uploaded_file.name}")
                log(f"  - ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°: {len(uploaded_overall)}å¹´åˆ†")
                log(f"  - å«ã¾ã‚Œã‚‹å¹´åº¦: {sorted(uploaded_years, key=_year_sort_key)}")
                for year, data in uploaded_overall.items():
                    log(f"    {year}å¹´: {len(data)}ç¤¾")
                    if data:
                        top = data[0]
                        log(f"      1ä½: {top.get('company')} ({top.get('score')}ç‚¹)")
                log(f"  - è©•ä¾¡é …ç›®åˆ¥: {len(uploaded_item)}é …ç›®")
                for item_name in list(uploaded_item.keys())[:3]:
                    log(f"    [{item_name}]")
                log(f"  - éƒ¨é–€åˆ¥: {len(uploaded_dept)}éƒ¨é–€")
                for dept_name in list(uploaded_dept.keys())[:3]:
                    log(f"    [{dept_name}]")

            # Step 2: Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§éå»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆv7.9: withæ–‡ã§ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ï¼‰
            status_text.text("ğŸŒ Webã‹ã‚‰éå»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            progress_bar.progress(20)

            log(f"[INFO] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–: {ranking_slug} ({ranking_name})")

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡å¹´åº¦ã‚’æ±ºå®š
            # - ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å¹´åº¦ã¯é™¤å¤–
            # - Webã‚µã‚¤ãƒˆã®æœ€æ–°å¹´åº¦ï¼ˆcurrent_yearï¼‰ã‚’è¶…ãˆã‚‹å¹´åº¦ã¯é™¤å¤–
            scrape_years = []
            effective_end_year = min(year_range[1], current_year)  # Webã‚µã‚¤ãƒˆã®æœ€æ–°å¹´åº¦ã‚’è¶…ãˆãªã„
            for y in range(year_range[0], effective_end_year + 1):
                if y not in uploaded_years:
                    scrape_years.append(y)

            log(f"[INFO] å¹´åº¦ç¯„å›²è¨­å®š: {year_range[0]}ã€œ{year_range[1]}")
            log(f"[INFO] Webã‚µã‚¤ãƒˆæœ€æ–°å¹´åº¦: {current_year}")
            log(f"[INFO] ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¹´åº¦: {sorted(uploaded_years, key=_year_sort_key) if uploaded_years else 'ãªã—'}")

            if scrape_years:
                log(f"[INFO] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡å¹´åº¦: {scrape_years}")
                scrape_range = (min(scrape_years, key=_year_sort_key), max(scrape_years, key=_year_sort_key))
            else:
                log(f"[INFO] ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã§å…¨å¹´åº¦ã‚«ãƒãƒ¼æ¸ˆã¿ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                scrape_range = None

            scraped_overall = {}
            scraped_item = {}
            scraped_dept = {}
            used_urls = None
            update_date = None

            # withæ–‡ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ï¼ˆè‡ªå‹•çš„ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒ­ãƒ¼ã‚ºï¼‰
            with OriconScraper(ranking_slug, ranking_name) as scraper:
                subpath_info = f" + subpath: {scraper.subpath}" if scraper.subpath else ""
                log(f"[INFO] URL prefix: {scraper.url_prefix}{subpath_info}")

                if scrape_range:
                    status_text.text(f"ğŸ“Š ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–å¾—ä¸­... ({scrape_range[0]}å¹´ã€œ{scrape_range[1]}å¹´)")
                    progress_bar.progress(30)

                    scraped_overall = scraper.get_overall_rankings(scrape_range)
                    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿å¹´åº¦ã‚’é™¤å¤–
                    scraped_overall = {y: d for y, d in scraped_overall.items() if y not in uploaded_years}
                    log(f"[OK] ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°: {len(scraped_overall)}å¹´åˆ†å–å¾—")
                    for year, data in scraped_overall.items():
                        log(f"  - {year}å¹´: {len(data)}ç¤¾")
                    progress_bar.progress(45)

                    status_text.text(f"ğŸ“‹ è©•ä¾¡é …ç›®åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
                    scraped_item = scraper.get_evaluation_items(scrape_range)
                    log(f"[OK] è©•ä¾¡é …ç›®åˆ¥: {len(scraped_item)}é …ç›®")
                    progress_bar.progress(60)

                    status_text.text(f"ğŸ·ï¸ éƒ¨é–€åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
                    scraped_dept = scraper.get_departments(scrape_range)
                    log(f"[OK] éƒ¨é–€åˆ¥: {len(scraped_dept)}éƒ¨é–€")
                    progress_bar.progress(70)

                    used_urls = scraper.used_urls

                # æ›´æ–°æ—¥ã‚’å–å¾—ï¼ˆæ¨å¥¨TOPICSã‚¿ãƒ–ã§ä½¿ç”¨ï¼‰
                update_date = scraper.get_update_date()

            # Step 3: ãƒ‡ãƒ¼ã‚¿çµ±åˆ
            status_text.text("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­...")
            progress_bar.progress(75)

            overall_data = merge_data(uploaded_overall, scraped_overall)
            item_data = merge_nested_data(uploaded_item, scraped_item)
            dept_data = merge_nested_data(uploaded_dept, scraped_dept)

            log(f"[OK] ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†")
            log(f"  - ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°: {len(overall_data)}å¹´åˆ†ï¼ˆçµ±åˆå¾Œï¼‰")
            log(f"    â”” ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: {len(uploaded_overall)}å¹´åˆ†")
            log(f"    â”” ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {len(scraped_overall)}å¹´åˆ†")

            # Step 4: åˆ†æå®Ÿè¡Œï¼ˆv5.8: éƒ¨é–€åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚‚æ¸¡ã™ï¼‰
            status_text.text("ğŸ” TOPICSåˆ†æä¸­...")
            analyzer = TopicsAnalyzer(overall_data, item_data, ranking_name, dept_data)
            topics = analyzer.analyze()
            progress_bar.progress(85)

            # Step 5: æ­´ä»£è¨˜éŒ²ãƒ»å¾—ç‚¹æ¨ç§»åˆ†æ
            status_text.text("ğŸ“ˆ æ­´ä»£è¨˜éŒ²ãƒ»å¾—ç‚¹æ¨ç§»ã‚’åˆ†æä¸­...")
            historical_analyzer = HistoricalAnalyzer(overall_data, item_data, dept_data, ranking_name)
            historical_data = historical_analyzer.analyze_all()
            # è©•ä¾¡é …ç›®åˆ¥ãƒ»éƒ¨é–€åˆ¥ã®1ä½ç²å¾—å›æ•°ã‚’è¨ˆç®—
            item_most_wins = historical_analyzer.calc_item_most_wins()
            dept_most_wins = historical_analyzer.calc_dept_most_wins()
            progress_bar.progress(95)

            # å®Œäº†
            status_text.text("âœ… å®Œäº†!")
            progress_bar.progress(100)

            # Step 6: åç§°å¤‰æ›´ã‚’æ¤œå‡º
            item_name_changes = detect_name_changes(used_urls, "items") if used_urls else {}
            dept_name_changes = detect_name_changes(used_urls, "departments") if used_urls else {}

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«çµæœã‚’ä¿å­˜
            st.session_state.results_data = {
                'ranking_name': ranking_name,
                'overall_data': overall_data,
                'item_data': item_data,
                'dept_data': dept_data,
                'historical_data': historical_data,
                'topics': topics,
                'used_urls': used_urls,
                'uploaded_years': list(uploaded_years),
                'scraped_years': list(scraped_overall.keys()) if scraped_overall else [],
                'item_most_wins': item_most_wins,
                'dept_most_wins': dept_most_wins,
                'item_name_changes': item_name_changes,
                'dept_name_changes': dept_name_changes,
                'update_date': update_date  # èª¿æŸ»æ¦‚è¦ã®æ›´æ–°æ—¥ï¼ˆå¹´, æœˆï¼‰
            }

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            logger.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}\n{error_detail}")
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            # ãƒ‡ãƒãƒƒã‚°ç”¨: ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’æŠ˜ã‚ŠãŸãŸã¿è¡¨ç¤ºï¼ˆv7.9: ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡ï¼‰
            # æœ¬ç•ªç’°å¢ƒã§ã¯SHOW_DEBUG_INFO=falseã«è¨­å®šã—ã¦ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’å‘ä¸Š
            if os.environ.get("SHOW_DEBUG_INFO", "false").lower() == "true":
                with st.expander("ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°ï¼ˆé–‹ç™ºè€…å‘ã‘ï¼‰", expanded=False):
                    st.code(error_detail, language="python")

# çµæœè¡¨ç¤ºï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ï¼‰
if st.session_state.results_data:
    data = st.session_state.results_data
    ranking_name = data['ranking_name']
    overall_data = data['overall_data']
    item_data = data['item_data']
    dept_data = data['dept_data']
    historical_data = data['historical_data']
    topics = data['topics']
    used_urls = data.get('used_urls')
    uploaded_years = data.get('uploaded_years', [])
    scraped_years = data.get('scraped_years', [])
    item_most_wins = data.get('item_most_wins', {})
    dept_most_wins = data.get('dept_most_wins', {})
    item_name_changes = data.get('item_name_changes', {})
    dept_name_changes = data.get('dept_name_changes', {})
    update_date = data.get('update_date')  # (year, month) ã®ã‚¿ãƒ—ãƒ«

    # çµæœè¡¨ç¤º
    st.success(f"âœ… {ranking_name}ã®TOPICSå‡ºã—ãŒå®Œäº†ã—ã¾ã—ãŸ")

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±
    if uploaded_years or scraped_years:
        col_info1, col_info2 = st.columns(2)
        with col_info1:
            if uploaded_years:
                st.info(f"ğŸ“ **ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿**: {sorted(uploaded_years, key=_year_sort_key)}å¹´")
        with col_info2:
            if scraped_years:
                st.info(f"ğŸŒ **Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°**: {sorted(scraped_years, key=_year_sort_key)}å¹´")

    # Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ï¼ˆå¤§ããç›®ç«‹ã¤ã‚ˆã†ã«ï¼‰
    st.markdown("---")
    excel_data = create_excel_export(
        ranking_name,
        overall_data,
        item_data,
        dept_data,
        historical_data,
        used_urls
    )

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.download_button(
            label="ğŸ“¥ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’Excelã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=excel_data,
            file_name=f"{ranking_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary",
            use_container_width=True,
            key="excel_download_main"
        )
    st.markdown("---")

    # æœ€æ–°å¹´åº¦ã‚’å–å¾—
    latest_year = max(overall_data.keys(), key=_year_sort_key) if overall_data else None
    # æ›´æ–°æ—¥ã‹ã‚‰å¹´æœˆã‚’å–å¾—ï¼ˆèª¿æŸ»æ¦‚è¦ã®æ›´æ–°æ—¥ãƒ™ãƒ¼ã‚¹ã€å–å¾—ã§ããªã„å ´åˆã¯ç¾åœ¨æ—¥æ™‚ã‚’ä½¿ç”¨ï¼‰
    if update_date:
        update_year, update_month = update_date
    else:
        update_year = latest_year if latest_year else datetime.now().year
        update_month = datetime.now().month

    # ã‚¿ãƒ–ã§çµæœè¡¨ç¤ºï¼ˆæ–°ã—ã„æ§‹æˆï¼‰
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        f"â­ æ¨å¥¨TOPICSï¼ˆ{update_year}å¹´{update_month}æœˆæ™‚ç‚¹ï¼‰" if update_year else "â­ æ¨å¥¨TOPICS",
        "ğŸ† æ­´ä»£è¨˜éŒ²ãƒ»å¾—ç‚¹æ¨ç§»",
        "ğŸ“Š ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°",
        "ğŸ“‹ è©•ä¾¡é …ç›®åˆ¥",
        "ğŸ·ï¸ éƒ¨é–€åˆ¥",
        "ğŸ“ ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ä½œæˆ",
        "ğŸ“ å‚è€ƒè³‡æ–™"
    ])

    with tab1:
        st.header(f"â­ æ¨å¥¨TOPICSï¼ˆ{update_year}å¹´{update_month}æœˆæ™‚ç‚¹ï¼‰" if update_year else "â­ æ¨å¥¨TOPICS")

        # v5.9: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«TOPICSã‚’åˆ†é¡ã—ã¦è¡¨ç¤º
        recommended_topics = topics.get("recommended", [])

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
        overall_topics = [t for t in recommended_topics if t.get("category") == "ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°"]
        item_topics = [t for t in recommended_topics if t.get("category") == "è©•ä¾¡é …ç›®åˆ¥"]
        dept_topics = [t for t in recommended_topics if t.get("category") == "éƒ¨é–€åˆ¥"]

        # ã‚«ãƒ†ã‚´ãƒªæœªè¨­å®šã®ã‚‚ã®ã¯ç·åˆã«åˆ†é¡ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
        other_categorized = [t for t in recommended_topics if t.get("category") not in ["ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°", "è©•ä¾¡é …ç›®åˆ¥", "éƒ¨é–€åˆ¥"]]
        overall_topics.extend(other_categorized)

        # ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if overall_topics:
            st.subheader("ğŸ“Š ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            for i, topic in enumerate(overall_topics, 1):
                st.markdown(f"**{i}. {topic['title']}**")
            st.divider()

        # è©•ä¾¡é …ç›®åˆ¥
        if item_topics:
            st.subheader("ğŸ“‹ è©•ä¾¡é …ç›®åˆ¥")
            for i, topic in enumerate(item_topics, 1):
                st.markdown(f"**{i}. {topic['title']}**")
            st.divider()

        # éƒ¨é–€åˆ¥
        if dept_topics:
            st.subheader("ğŸ·ï¸ éƒ¨é–€åˆ¥")
            for i, topic in enumerate(dept_topics, 1):
                st.markdown(f"**{i}. {topic['title']}**")
            st.divider()

        if topics.get("other"):
            st.subheader("ğŸ“ ãã®ä»–ã®TOPICSå€™è£œ")
            for topic in topics["other"]:
                st.markdown(f"- {topic}")

        # è¦‹å‡ºã—æ¡ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ¨å¥¨TOPICSã‚¿ãƒ–å†…ã«çµ±åˆï¼‰
        st.divider()
        st.subheader("ğŸ¯ è¦‹å‡ºã—æ¡ˆ")
        for i, headline in enumerate(topics.get("headlines", []), 1):
            st.markdown(f"**ãƒ‘ã‚¿ãƒ¼ãƒ³{i}**: {headline}")

        # ã‚³ãƒ”ãƒ¼ç”¨ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†ï¼‰
        st.subheader("ğŸ“‹ ã‚³ãƒ”ãƒ¼ç”¨ãƒ†ã‚­ã‚¹ãƒˆ")
        copy_lines = [f"ã€æ¨å¥¨TOPICSï¼ˆ{update_year}å¹´{update_month}æœˆæ™‚ç‚¹ï¼‰ã€‘" if update_year else "ã€æ¨å¥¨TOPICSã€‘"]

        if overall_topics:
            copy_lines.append("\nâ–  ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            copy_lines.extend([f"ãƒ»{t['title']}" for t in overall_topics])
        if item_topics:
            copy_lines.append("\nâ–  è©•ä¾¡é …ç›®åˆ¥")
            copy_lines.extend([f"ãƒ»{t['title']}" for t in item_topics])
        if dept_topics:
            copy_lines.append("\nâ–  éƒ¨é–€åˆ¥")
            copy_lines.extend([f"ãƒ»{t['title']}" for t in dept_topics])

        copy_lines.append("\nã€è¦‹å‡ºã—æ¡ˆã€‘")
        copy_lines.extend([f"ãƒ‘ã‚¿ãƒ¼ãƒ³{i}: {h}" for i, h in enumerate(topics.get("headlines", []), 1)])

        copy_text = "\n".join(copy_lines)
        st.text_area("ã‚³ãƒ”ãƒ¼ç”¨", copy_text, height=350, label_visibility="collapsed")

    with tab2:
        st.header("ğŸ† æ­´ä»£è¨˜éŒ²ãƒ»å¾—ç‚¹æ¨ç§»")
        records = historical_data.get("historical_records", {})
        trends = historical_data.get("score_trends", {})

        if records:
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            display_historical_summary(records)
            st.divider()

            # 2ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
            col_left, col_right = st.columns(2)

            with col_left:
                # é€£ç¶š1ä½è¨˜éŒ²ï¼ˆ2å¹´ä»¥ä¸Šã®ã¿ï¼‰
                st.subheader("ğŸ¥‡ é€£ç¶š1ä½è¨˜éŒ²")
                consecutive = records.get("consecutive_wins", [])
                if consecutive:
                    # 2å¹´ä»¥ä¸Šã®è¨˜éŒ²ã®ã¿è¡¨ç¤º
                    consecutive_filtered = [r for r in consecutive if r.get("years", 0) >= 2]
                    if consecutive_filtered:
                        cons_df = pd.DataFrame([
                            {
                                "ä¼æ¥­å": r["company"],
                                "é€£ç¶šå¹´æ•°": f"{r['years']}å¹´",
                                "æœŸé–“": f"{r['start_year']}ã€œ{r['end_year']}",
                                "ç¶™ç¶šä¸­": "âœ…" if r.get("is_current") else ""
                            }
                            for r in consecutive_filtered[:10]
                        ])
                        st.dataframe(cons_df, use_container_width=True, hide_index=True)

                # éå»æœ€é«˜å¾—ç‚¹
                st.subheader("ğŸ“ˆ éå»æœ€é«˜å¾—ç‚¹TOP10")
                highest = records.get("highest_scores", [])
                if highest:
                    high_df = pd.DataFrame([
                        {
                            "é †ä½": i,
                            "ä¼æ¥­å": r["company"],
                            "å¾—ç‚¹": f"{r['score']}ç‚¹",
                            "å¹´åº¦": f"{r['year']}å¹´",
                            "ãã®å¹´ã®é †ä½": f"{r['rank']}ä½"
                        }
                        for i, r in enumerate(highest[:10], 1)
                    ])
                    st.dataframe(high_df, use_container_width=True, hide_index=True)

            with col_right:
                # æœ€å¤š1ä½ç²å¾—
                st.subheader("ğŸ† 1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                most_wins = records.get("most_wins", [])
                if most_wins:
                    wins_df = pd.DataFrame([
                        {
                            "ä¼æ¥­å": r["company"],
                            "1ä½å›æ•°": f"{r['wins']}å›",
                            "ç²å¾—å¹´": ", ".join(map(str, r["years"]))
                        }
                        for r in most_wins[:10]
                    ])
                    st.dataframe(wins_df, use_container_width=True, hide_index=True)

                # å¹´åº¦åˆ¥1ä½ã®æ¨ç§»
                st.subheader("ğŸ¥‡ å¹´åº¦åˆ¥1ä½ã®æ¨ç§»")
                top_by_year = trends.get("top_score_by_year", {})
                if top_by_year:
                    top_df = pd.DataFrame([
                        {
                            "å¹´åº¦": year,
                            "1ä½ä¼æ¥­": top_by_year[year].get("company", "-") if isinstance(top_by_year.get(year), dict) else "-",
                            "å¾—ç‚¹": f"{top_by_year[year].get('score', '-')}ç‚¹" if isinstance(top_by_year.get(year), dict) else "-"
                        }
                        for year in sorted(top_by_year.keys(), key=_year_sort_key, reverse=True)
                    ])
                    st.dataframe(top_df, use_container_width=True, hide_index=True)

        st.divider()

        # å¾—ç‚¹æ¨ç§»ã‚°ãƒ©ãƒ•
        if trends and trends.get("years"):
            years = trends["years"]

            # å¹´åº¦åˆ¥å¹³å‡å¾—ç‚¹
            st.subheader("ğŸ“Š å¹´åº¦åˆ¥å¹³å‡å¾—ç‚¹ã®æ¨ç§»")
            avg_scores = trends.get("average_scores", {})
            if avg_scores:
                avg_df = pd.DataFrame([
                    {"å¹´åº¦": year, "å¹³å‡å¾—ç‚¹": score}
                    for year, score in sorted(avg_scores.items(), key=lambda x: _year_sort_key(x[0]))
                ])
                import altair as alt
                # å‹•çš„Yè»¸ç¯„å›²ï¼ˆæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã¯zero=FalseãŒæœ‰åŠ¹ï¼‰
                score_values = list(avg_scores.values())
                y_min = max(0, min(score_values) - 3)
                y_max = max(score_values) + 3
                chart = alt.Chart(avg_df).mark_line(point=True).encode(
                    x=alt.X('å¹´åº¦:O', title='å¹´åº¦'),
                    y=alt.Y('å¹³å‡å¾—ç‚¹:Q', title='å¹³å‡å¾—ç‚¹', scale=alt.Scale(domain=[y_min, y_max]))
                ).properties(height=300)
                st.altair_chart(chart, use_container_width=True)

            # ä¸Šä½ä¼æ¥­ã®å¾—ç‚¹æ¨ç§»
            st.subheader("ğŸ“ˆ ä¸Šä½ä¼æ¥­ã®å¾—ç‚¹æ¨ç§»")
            top_companies = trends.get("top_companies", [])[:5]
            companies_data = trends.get("companies", {})

            if top_companies and companies_data:
                chart_data = []
                for company in top_companies:
                    if company in companies_data:
                        for year in years:
                            score = companies_data[company].get(year, {}).get("score")
                            if score:
                                chart_data.append({
                                    "å¹´åº¦": str(year),
                                    "ä¼æ¥­å": company,
                                    "å¾—ç‚¹": score
                                })

                if chart_data:
                    chart_df = pd.DataFrame(chart_data)
                    # å‹•çš„Yè»¸ç¯„å›²
                    all_scores = [d["å¾—ç‚¹"] for d in chart_data]
                    y_min = max(0, min(all_scores) - 3)
                    y_max = max(all_scores) + 3
                    chart = alt.Chart(chart_df).mark_line(point=True).encode(
                        x=alt.X('å¹´åº¦:O', title='å¹´åº¦'),
                        y=alt.Y('å¾—ç‚¹:Q', title='å¾—ç‚¹', scale=alt.Scale(domain=[y_min, y_max])),
                        color=alt.Color('ä¼æ¥­å:N', title='ä¼æ¥­å'),
                        tooltip=['å¹´åº¦', 'ä¼æ¥­å', 'å¾—ç‚¹']
                    ).properties(height=400)
                    st.altair_chart(chart, use_container_width=True)

            # v7.3: è©•ä¾¡é …ç›®åˆ¥ãƒ»éƒ¨é–€åˆ¥ å¹³å‡å¾—ç‚¹æ¨ç§»ã‚’ã€Œä¸Šä½ä¼æ¥­ã®å¾—ç‚¹æ¨ç§»ã€ã®ä¸‹ã«ç§»å‹•
            # è©•ä¾¡é …ç›®åˆ¥ å¹³å‡å¾—ç‚¹æ¨ç§»ï¼ˆç¸¦æ£’ã‚°ãƒ©ãƒ•ï¼‰- trendsã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšè¡¨ç¤º
            st.divider()
            st.subheader("ğŸ“Š è©•ä¾¡é …ç›®åˆ¥ å¹³å‡å¾—ç‚¹æ¨ç§»")
            if item_data:
                # å„è©•ä¾¡é …ç›®ã®å¹´åº¦åˆ¥å¹³å‡å¾—ç‚¹ã‚’è¨ˆç®—
                item_avg_data = []
                for item_name, year_data in item_data.items():
                    if isinstance(year_data, dict):
                        for year, data in year_data.items():
                            # 0ç‚¹ã‚‚æœ‰åŠ¹ãªå€¤ã¨ã—ã¦æ‰±ã†ï¼ˆNoneã®ã¿ã‚’é™¤å¤–ï¼‰
                            scores = [d.get("score") for d in data if d.get("score") is not None]
                            if scores:
                                item_avg_data.append({
                                    "è©•ä¾¡é …ç›®": item_name[:15],  # é•·ã™ãã‚‹é …ç›®åã‚’çŸ­ç¸®
                                    "å¹´åº¦": str(year),
                                    "å¹³å‡å¾—ç‚¹": round(sum(scores) / len(scores), 2)
                                })

                if item_avg_data:
                    item_avg_df = pd.DataFrame(item_avg_data)
                    # æœ€æ–°3å¹´åº¦ã«çµã‚‹
                    latest_years = sorted(item_avg_df["å¹´åº¦"].unique(), key=_year_sort_key, reverse=True)[:3]
                    item_avg_df = item_avg_df[item_avg_df["å¹´åº¦"].isin(latest_years)]

                    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ç¸¦æ£’ã‚°ãƒ©ãƒ•ï¼ˆå¹´åº¦ã”ã¨ã«æ¨ªä¸¦ã³ï¼‰- mark_rectã§é0åŸºç‚¹
                    import altair as alt
                    all_scores = item_avg_df["å¹³å‡å¾—ç‚¹"].tolist()
                    y_min = max(0, min(all_scores) - 5)
                    y_max = max(all_scores) + 2
                    item_avg_df["åŸºç‚¹"] = y_min
                    chart = alt.Chart(item_avg_df).mark_rect(width=12).encode(
                        x=alt.X('å¹´åº¦:N', title=None, axis=alt.Axis(labelAngle=0)),
                        y=alt.Y('åŸºç‚¹:Q', title='å¹³å‡å¾—ç‚¹', scale=alt.Scale(domain=[y_min, y_max])),
                        y2=alt.Y2('å¹³å‡å¾—ç‚¹:Q'),
                        color=alt.Color('å¹´åº¦:N', title='å¹´åº¦'),
                        column=alt.Column('è©•ä¾¡é …ç›®:N', title=None, header=alt.Header(labelOrient='bottom')),
                        tooltip=['è©•ä¾¡é …ç›®', 'å¹´åº¦', 'å¹³å‡å¾—ç‚¹']
                    ).properties(width=60, height=400)
                    st.altair_chart(chart)
                else:
                    st.info("è©•ä¾¡é …ç›®åˆ¥ãƒ‡ãƒ¼ã‚¿ã«ã‚¹ã‚³ã‚¢ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            else:
                st.info("è©•ä¾¡é …ç›®åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # éƒ¨é–€åˆ¥ å¹³å‡å¾—ç‚¹æ¨ç§»ï¼ˆç¸¦æ£’ã‚°ãƒ©ãƒ•ï¼‰- trendsã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšè¡¨ç¤º
            st.subheader("ğŸ“Š éƒ¨é–€åˆ¥ å¹³å‡å¾—ç‚¹æ¨ç§»")
            if dept_data:
                dept_avg_data = []
                for dept_name, year_data in dept_data.items():
                    if isinstance(year_data, dict):
                        for year, data in year_data.items():
                            scores = [d.get("score") for d in data if d.get("score") is not None]
                            if scores:
                                dept_avg_data.append({
                                    "éƒ¨é–€": dept_name[:15],
                                    "å¹´åº¦": str(year),
                                    "å¹³å‡å¾—ç‚¹": round(sum(scores) / len(scores), 2)
                                })

                if dept_avg_data:
                    dept_avg_df = pd.DataFrame(dept_avg_data)
                    latest_years = sorted(dept_avg_df["å¹´åº¦"].unique(), key=_year_sort_key, reverse=True)[:3]
                    dept_avg_df = dept_avg_df[dept_avg_df["å¹´åº¦"].isin(latest_years)]

                    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ç¸¦æ£’ã‚°ãƒ©ãƒ•ï¼ˆå¹´åº¦ã”ã¨ã«æ¨ªä¸¦ã³ï¼‰- mark_rectã§é0åŸºç‚¹
                    import altair as alt
                    all_scores = dept_avg_df["å¹³å‡å¾—ç‚¹"].tolist()
                    y_min = max(0, min(all_scores) - 5)
                    y_max = max(all_scores) + 2
                    dept_avg_df["åŸºç‚¹"] = y_min
                    chart = alt.Chart(dept_avg_df).mark_rect(width=12).encode(
                        x=alt.X('å¹´åº¦:N', title=None, axis=alt.Axis(labelAngle=0)),
                        y=alt.Y('åŸºç‚¹:Q', title='å¹³å‡å¾—ç‚¹', scale=alt.Scale(domain=[y_min, y_max])),
                        y2=alt.Y2('å¹³å‡å¾—ç‚¹:Q'),
                        color=alt.Color('å¹´åº¦:N', title='å¹´åº¦'),
                        column=alt.Column('éƒ¨é–€:N', title=None, header=alt.Header(labelOrient='bottom')),
                        tooltip=['éƒ¨é–€', 'å¹´åº¦', 'å¹³å‡å¾—ç‚¹']
                    ).properties(width=60, height=400)
                    st.altair_chart(chart)
                else:
                    st.info("éƒ¨é–€åˆ¥ãƒ‡ãƒ¼ã‚¿ã«ã‚¹ã‚³ã‚¢ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            else:
                st.info("éƒ¨é–€åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # è©•ä¾¡é …ç›®åˆ¥ã®é€£ç¶š1ä½
            st.subheader("ğŸ“‹ è©•ä¾¡é …ç›®åˆ¥ é€£ç¶š1ä½è¨˜éŒ²")
            item_trends = historical_data.get("item_trends", {})
            if item_trends:
                item_records = []
                for item_name, data in item_trends.items():
                    for streak in data.get("consecutive_wins", []):
                        if streak.get("years", 0) >= 2:
                            item_records.append({
                                "è©•ä¾¡é …ç›®": item_name,
                                "ä¼æ¥­å": streak["company"],
                                "é€£ç¶šå¹´æ•°": f"{streak['years']}å¹´",
                                "æœŸé–“": f"{streak['start']}ã€œ{streak['end']}",
                                "ç¶™ç¶šä¸­": "âœ…" if streak.get("is_current") else ""
                            })
                if item_records:
                    item_records.sort(key=lambda x: -int(x["é€£ç¶šå¹´æ•°"].replace("å¹´", "")))
                    st.dataframe(pd.DataFrame(item_records[:15]), use_container_width=True, hide_index=True)

            # éƒ¨é–€åˆ¥ã®é€£ç¶š1ä½
            st.subheader("ğŸ·ï¸ éƒ¨é–€åˆ¥ é€£ç¶š1ä½è¨˜éŒ²")
            dept_trends = historical_data.get("dept_trends", {})
            if dept_trends:
                dept_records = []
                for dept_name, data in dept_trends.items():
                    for streak in data.get("consecutive_wins", []):
                        if streak.get("years", 0) >= 2:
                            dept_records.append({
                                "éƒ¨é–€": dept_name,
                                "ä¼æ¥­å": streak["company"],
                                "é€£ç¶šå¹´æ•°": f"{streak['years']}å¹´",
                                "æœŸé–“": f"{streak['start']}ã€œ{streak['end']}",
                                "ç¶™ç¶šä¸­": "âœ…" if streak.get("is_current") else ""
                            })
                if dept_records:
                    dept_records.sort(key=lambda x: -int(x["é€£ç¶šå¹´æ•°"].replace("å¹´", "")))
                    st.dataframe(pd.DataFrame(dept_records[:15]), use_container_width=True, hide_index=True)

    with tab3:
        st.header("ğŸ“Š ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆçµŒå¹´è©³ç´°ï¼‰")

        # ãƒˆãƒƒãƒ—ã«æ­´ä»£è¨˜éŒ²ã‚’è¡¨ç¤º
        records = historical_data.get("historical_records", {})
        if records:
            display_historical_summary(records)
            display_consecutive_wins_compact(records)
            st.divider()

        # ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        most_wins = records.get("most_wins", []) if records else []
        if most_wins:
            st.subheader("ğŸ† ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚° 1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            # æœ€æ–°å¹´åº¦ã‚’å–å¾—
            all_years = set()
            for r in most_wins:
                all_years.update(r.get("years", []))
            latest_year = max(all_years, key=_year_sort_key) if all_years else None

            overall_wins_data = []
            for r in most_wins:
                if r.get("wins", 0) > 0:
                    # ç¶™ç¶šä¸­ãƒ•ãƒ©ã‚°: æœ€æ–°å¹´åº¦ã‚‚1ä½ãªã‚‰âœ…
                    is_current = latest_year in r.get("years", []) if latest_year else False
                    wins = r.get("wins", 0)
                    overall_wins_data.append({
                        "ä¼æ¥­å": r.get("company", ""),
                        "1ä½å›æ•°": wins,  # ã‚½ãƒ¼ãƒˆç”¨ã«æ•°å€¤ã§ä¿æŒ
                        "ç¶™ç¶šä¸­": "âœ…" if is_current else "",
                        "ç²å¾—å¹´": ", ".join(map(str, r.get("years", [])))
                    })
            if overall_wins_data:
                # 1ä½å›æ•°ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆ
                overall_wins_data.sort(key=lambda x: -x["1ä½å›æ•°"])
                # è¡¨ç¤ºç”¨ã«å›æ•°ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                for d in overall_wins_data:
                    d["1ä½å›æ•°"] = f"{d['1ä½å›æ•°']}å›"
                st.dataframe(pd.DataFrame(overall_wins_data), use_container_width=True, hide_index=True)

        # v7.3: ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP10å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ã‚’ã€Œ1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ã®ä¸‹ã«ç§»å‹•
        if overall_data and len(overall_data) > 1:
            st.subheader("ğŸ“Š å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ï¼ˆTOP10ä¼æ¥­ï¼‰")
            # æœ€æ–°å¹´åº¦ã®TOP10ä¼æ¥­ã‚’å–å¾—
            latest_year_for_chart = max(overall_data.keys(), key=_year_sort_key)
            latest_top10_for_chart = sorted(overall_data[latest_year_for_chart], key=lambda x: x.get("score") or 0, reverse=True)[:10]
            top10_companies_for_chart = [d.get("company") for d in latest_top10_for_chart if d.get("company")]

            line_data_for_chart = []
            for year in sorted(overall_data.keys(), key=_year_sort_key):
                for item in overall_data[year]:
                    company = item.get("company")
                    score = item.get("score")
                    if company in top10_companies_for_chart and score is not None:
                        line_data_for_chart.append({
                            "å¹´åº¦": str(year),
                            "å¾—ç‚¹": score,
                            "ä¼æ¥­å": company[:15]  # é•·ã„ä¼æ¥­åã‚’çŸ­ç¸®
                        })
            if line_data_for_chart and len(line_data_for_chart) > 1:
                import altair as alt
                line_df_for_chart = pd.DataFrame(line_data_for_chart)
                # å‹•çš„Yè»¸ç¯„å›²
                all_scores_for_chart = [d["å¾—ç‚¹"] for d in line_data_for_chart]
                y_min_for_chart = max(0, min(all_scores_for_chart) - 3)
                y_max_for_chart = max(all_scores_for_chart) + 3
                chart = alt.Chart(line_df_for_chart).mark_line(point=True).encode(
                    x=alt.X('å¹´åº¦:O', title='å¹´åº¦'),
                    y=alt.Y('å¾—ç‚¹:Q', title='å¾—ç‚¹', scale=alt.Scale(domain=[y_min_for_chart, y_max_for_chart])),
                    color=alt.Color('ä¼æ¥­å:N', title='ä¼æ¥­å'),
                    tooltip=['å¹´åº¦', 'ä¼æ¥­å', 'å¾—ç‚¹']
                ).properties(height=400, title="ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚° å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ï¼ˆTOP10ä¼æ¥­ï¼‰")
                st.altair_chart(chart, use_container_width=True)

        st.divider()

        if overall_data:
            # å¹´åº¦ã”ã¨ã«å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¯ï¼‰
            for year in sorted(overall_data.keys(), key=_year_sort_key, reverse=True):
                source_mark = "ğŸ“" if year in uploaded_years else "ğŸŒ"
                # è©²å½“å¹´åº¦ã®URLã‚’å–å¾—
                year_url = None
                if used_urls:
                    for url_item in used_urls.get("overall", []):
                        if url_item.get("year") == year and url_item.get("status") == "success":
                            year_url = url_item.get("url", "")
                            break
                # expanderã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆURLã¯ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ã«ã™ã‚‹ãŸã‚ä¸­ã«è¡¨ç¤ºï¼‰
                expander_title = f"{source_mark} {year}å¹´"
                with st.expander(expander_title, expanded=(year == max(overall_data.keys(), key=_year_sort_key))):
                    # URLã‚’è¡¨ã®ä¸Šã«ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒªãƒ³ã‚¯ã¨ã—ã¦è¡¨ç¤º
                    if year_url:
                        st.markdown(f"ğŸ”— **å‚ç…§URL**: [{year_url}]({year_url})")
                    df = pd.DataFrame(overall_data[year])
                    # v7.3: ç©ºç™½åˆ—åã€æ•°å­—ã®ã¿ã®åˆ—åã€Unnamedåˆ—ã‚’é™¤å¤–
                    valid_cols = [col for col in df.columns
                                  if col and str(col).strip()
                                  and not str(col).strip().isdigit()
                                  and not str(col).startswith('Unnamed')]
                    df = df[valid_cols]
                    st.dataframe(df, use_container_width=True, hide_index=True)

                    # è©²å½“å¹´åº¦ã®ç¸¦æ£’ã‚°ãƒ©ãƒ•ï¼ˆå¾—ç‚¹ä¸Šä½10ç¤¾ï¼‰
                    year_data_sorted = sorted(overall_data[year], key=lambda x: x.get("score") or 0, reverse=True)[:10]
                    if year_data_sorted and any(d.get("score") for d in year_data_sorted):
                        import altair as alt
                        bar_data = []
                        for d in year_data_sorted:
                            if d.get("score") is not None and d.get("company"):
                                bar_data.append({
                                    "ä¼æ¥­å": d["company"][:12],  # é•·ã„ä¼æ¥­åã‚’çŸ­ç¸®
                                    "å¾—ç‚¹": d["score"]
                                })
                        if bar_data:
                            bar_df = pd.DataFrame(bar_data)
                            # mark_rectã§é0åŸºç‚¹ã®æ£’ã‚°ãƒ©ãƒ•ã‚’å®Ÿè£…ï¼ˆå·®åˆ†ã‚’è¦‹ã‚„ã™ãï¼‰
                            scores = [d["å¾—ç‚¹"] for d in bar_data]
                            y_min = max(0, min(scores) - 5)  # æœ€å°å€¤-5ã‚’åŸºç‚¹ã«
                            y_max = max(scores) + 2
                            bar_df["åŸºç‚¹"] = y_min
                            chart = alt.Chart(bar_df).mark_rect(width=25).encode(
                                x=alt.X('ä¼æ¥­å:N', sort=alt.EncodingSortField(field='å¾—ç‚¹', order='descending'), title=None, axis=alt.Axis(labelAngle=-45)),
                                y=alt.Y('åŸºç‚¹:Q', title='å¾—ç‚¹', scale=alt.Scale(domain=[y_min, y_max])),
                                y2=alt.Y2('å¾—ç‚¹:Q'),
                                color=alt.Color('å¾—ç‚¹:Q', scale=alt.Scale(scheme='blues'), legend=None),
                                tooltip=['ä¼æ¥­å', 'å¾—ç‚¹']
                            ).properties(height=300, title=f"{year}å¹´ å¾—ç‚¹ä¸Šä½10ç¤¾")
                            st.altair_chart(chart, use_container_width=True)

            # çµŒå¹´æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
            st.subheader("ğŸ“ˆ çµŒå¹´æ¯”è¼ƒï¼ˆå…¨ç¤¾å¾—ç‚¹æ¨ç§»ï¼‰")

            companies = set()
            for year_data in overall_data.values():
                for item in year_data:
                    companies.add(item.get("company", ""))

            comparison_data = []
            for company in sorted(companies):
                row = {"ä¼æ¥­å": company}
                for year in sorted(overall_data.keys(), key=_year_sort_key):
                    score = "-"
                    rank = "-"
                    for item in overall_data[year]:
                        if item.get("company") == company:
                            score = item.get("score", "-")
                            rank = item.get("rank", "-")
                            break
                    row[f"{year}å¹´å¾—ç‚¹"] = score
                    row[f"{year}å¹´é †ä½"] = rank
                comparison_data.append(row)

            if comparison_data:
                st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)

    with tab4:
        st.header("ğŸ“‹ è©•ä¾¡é …ç›®åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆçµŒå¹´ï¼‰")

        # è©•ä¾¡é …ç›®åˆ¥1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if item_most_wins:
            st.subheader("ğŸ† è©•ä¾¡é …ç›®åˆ¥ 1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            # æœ€æ–°å¹´åº¦ã‚’å–å¾—
            all_years = set()
            for wins_list in item_most_wins.values():
                for r in wins_list:
                    all_years.update(r.get("years", []))
            latest_year = max(all_years, key=_year_sort_key) if all_years else None

            item_wins_data = []
            for item_name, wins_list in item_most_wins.items():
                for r in wins_list[:3]:  # å„é …ç›®ä¸Šä½3ç¤¾
                    if r["wins"] > 0:
                        # ç¶™ç¶šä¸­ãƒ•ãƒ©ã‚°: æœ€æ–°å¹´åº¦ã‚‚1ä½ãªã‚‰âœ…
                        is_current = latest_year in r.get("years", []) if latest_year else False
                        item_wins_data.append({
                            "è©•ä¾¡é …ç›®": item_name,
                            "ä¼æ¥­å": r["company"],
                            "1ä½å›æ•°": r['wins'],  # ã‚½ãƒ¼ãƒˆç”¨ã«æ•°å€¤ã§ä¿æŒ
                            "ç²å¾—ç‡": f"{r['wins']/r['total_years']*100:.1f}%" if r['total_years'] > 0 else "0.0%",
                            "ç¶™ç¶šä¸­": "âœ…" if is_current else "",
                            "ç²å¾—å¹´": ", ".join(map(str, r["years"]))
                        })
            if item_wins_data:
                # 1ä½å›æ•°ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆ
                item_wins_data.sort(key=lambda x: -x["1ä½å›æ•°"])
                # è¡¨ç¤ºç”¨ã«å›æ•°ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                for d in item_wins_data:
                    d["1ä½å›æ•°"] = f"{d['1ä½å›æ•°']}å›"
                st.dataframe(pd.DataFrame(item_wins_data), use_container_width=True, hide_index=True)
            st.divider()

        # ãƒˆãƒƒãƒ—ã«è©•ä¾¡é …ç›®åˆ¥ã®é€£ç¶š1ä½è¨˜éŒ²
        item_trends = historical_data.get("item_trends", {})
        if item_trends:
            st.subheader("ğŸ“‹ è©•ä¾¡é …ç›®åˆ¥ é€£ç¶š1ä½è¨˜éŒ²ï¼ˆä¸Šä½10ä»¶ï¼‰")
            item_records = []
            for item_name, data in item_trends.items():
                for streak in data.get("consecutive_wins", []):
                    # è¤‡æ•°å›1ä½ã‚’ç²å¾—ã—ãŸã‚‚ã®ã®ã¿å¯¾è±¡ï¼ˆ1å¹´ã ã‘ã®å—è³ã¯é™¤å¤–ï¼‰
                    if streak.get("years", 0) >= 2:
                        item_records.append({
                            "è©•ä¾¡é …ç›®": item_name,
                            "ä¼æ¥­å": streak["company"],
                            "é€£ç¶šå¹´æ•°": f"{streak['years']}å¹´",
                            "æœŸé–“": f"{streak['start']}ã€œ{streak['end']}",
                            "ç¶™ç¶šä¸­": "âœ…" if streak.get("is_current") else ""
                        })
            if item_records:
                item_records.sort(key=lambda x: -int(x["é€£ç¶šå¹´æ•°"].replace("å¹´", "")))
                st.dataframe(pd.DataFrame(item_records[:10]), use_container_width=True, hide_index=True)
            st.divider()

        # v7.3: ã€Œè©•ä¾¡é …ç›®åˆ¥ å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ï¼ˆTOP10ä¼æ¥­ï¼‰ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤

        if item_data:
            for item_name, year_data in item_data.items():
                # æœ€æ–°åç§°ã‚’å–å¾—ï¼ˆåç§°å¤‰æ›´æƒ…å ±ãŒã‚ã‚Œã°ä½¿ç”¨ï¼‰
                display_name = item_name
                name_change_info = item_name_changes.get(item_name)
                if name_change_info and name_change_info.get("latest_name"):
                    display_name = name_change_info["latest_name"]

                with st.expander(f"ğŸ“Œ {display_name}", expanded=False):
                    # v6.1: ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå¤‰æ›´ - åç§°å¤‰æ›´ â†’ 1ä½ã®æ¨ç§» â†’ çµŒå¹´æ¨ç§» â†’ å¹´æ•°/URL ã®é †

                    # 1. åç§°å¤‰æ›´ãŒã‚ã‚Œã°æ³¨è¨˜ã‚’è¡¨ç¤º
                    if name_change_info and name_change_info.get("changes"):
                        for change in name_change_info["changes"]:
                            st.info(f"ğŸ“ **åç§°å¤‰æ›´**: {change['change_year']}å¹´ã‚ˆã‚Šã€Œ{change['from_name']}ã€â†’ã€Œ{change['to_name']}ã€ã«å¤‰æ›´")

                    if isinstance(year_data, dict) and len(year_data) > 1:
                        # 2. 1ä½ã®æ¨ç§»ï¼ˆåç§°å¤‰æ›´ã®ç›´å¾Œã«é…ç½®ï¼‰
                        st.markdown("**ğŸ“ˆ 1ä½ã®æ¨ç§»**")
                        history = []
                        for year in sorted(year_data.keys(), key=_year_sort_key, reverse=True):
                            year_list = year_data.get(year)
                            if year_list and isinstance(year_list, list) and len(year_list) > 0:
                                top = year_list[0]
                                if top and isinstance(top, dict):
                                    history.append({
                                        "å¹´åº¦": year,
                                        "1ä½": top.get("company", "-"),
                                        "å¾—ç‚¹": top.get("score", "-")
                                    })
                        if history:
                            st.dataframe(pd.DataFrame(history), use_container_width=True)

                        # 3. çµŒå¹´å¤‰åŒ–ã®æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ï¼ˆTOP10ä¼æ¥­ã®å¾—ç‚¹æ¨ç§»ï¼‰
                        st.markdown("**ğŸ“Š å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ï¼ˆTOP10ä¼æ¥­ï¼‰**")
                        # æœ€æ–°å¹´åº¦ã®TOP10ä¼æ¥­ã‚’å–å¾—
                        latest_yr = max(year_data.keys(), key=_year_sort_key)
                        latest_top10 = sorted(year_data[latest_yr], key=lambda x: x.get("score") or 0, reverse=True)[:10]
                        top10_companies = [d.get("company") for d in latest_top10 if d.get("company")]

                        line_data = []
                        for yr in sorted(year_data.keys(), key=_year_sort_key):
                            for item in year_data[yr]:
                                company = item.get("company")
                                score = item.get("score")
                                if company in top10_companies and score is not None:
                                    line_data.append({
                                        "å¹´åº¦": str(yr),
                                        "å¾—ç‚¹": score,
                                        "ä¼æ¥­å": company[:15]
                                    })
                        if line_data and len(line_data) > 1:
                            import altair as alt
                            line_df = pd.DataFrame(line_data)
                            # å‹•çš„Yè»¸ç¯„å›²
                            all_scores = [d["å¾—ç‚¹"] for d in line_data]
                            y_min = max(0, min(all_scores) - 3)
                            y_max = max(all_scores) + 3
                            chart = alt.Chart(line_df).mark_line(point=True).encode(
                                x=alt.X('å¹´åº¦:O', title='å¹´åº¦'),
                                y=alt.Y('å¾—ç‚¹:Q', title='å¾—ç‚¹', scale=alt.Scale(domain=[y_min, y_max])),
                                color=alt.Color('ä¼æ¥­å:N', title='ä¼æ¥­å'),
                                tooltip=['å¹´åº¦', 'ä¼æ¥­å', 'å¾—ç‚¹']
                            ).properties(height=300, title=f"{item_name} å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ï¼ˆTOP10ä¼æ¥­ï¼‰")
                            st.altair_chart(chart, use_container_width=True)

                        st.divider()

                        # 4. å„å¹´åº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆå¹´æ•°/URLï¼‰
                        for year in sorted(year_data.keys(), key=_year_sort_key, reverse=True):
                            # è©²å½“å¹´åº¦ã®URLã‚’å–å¾—
                            year_url = None
                            if used_urls:
                                for url_item in used_urls.get("items", []):
                                    search_name = f"{item_name}({year}å¹´)"
                                    if url_item.get("name") == search_name and url_item.get("status") == "success":
                                        year_url = url_item.get("url", "")
                                        break
                            # å¹´åº¦ã®æ¨ªã«URLè¡¨ç¤º
                            if year_url:
                                st.markdown(f"**{year}å¹´** ğŸ”— {year_url}")
                            else:
                                st.markdown(f"**{year}å¹´**")
                            df = pd.DataFrame(year_data[year])
                            # v7.3: ç©ºç™½åˆ—åã€æ•°å­—ã®ã¿ã®åˆ—åã€Unnamedåˆ—ã‚’é™¤å¤–
                            valid_cols = [col for col in df.columns
                                          if col and str(col).strip()
                                          and not str(col).strip().isdigit()
                                          and not str(col).startswith('Unnamed')]
                            df = df[valid_cols]
                            st.dataframe(df, use_container_width=True, hide_index=True)

                    elif isinstance(year_data, dict):
                        # 1å¹´åˆ†ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿
                        for year in sorted(year_data.keys(), key=_year_sort_key, reverse=True):
                            year_url = None
                            if used_urls:
                                for url_item in used_urls.get("items", []):
                                    search_name = f"{item_name}({year}å¹´)"
                                    if url_item.get("name") == search_name and url_item.get("status") == "success":
                                        year_url = url_item.get("url", "")
                                        break
                            if year_url:
                                st.markdown(f"**{year}å¹´** ğŸ”— {year_url}")
                            else:
                                st.markdown(f"**{year}å¹´**")
                            df = pd.DataFrame(year_data[year])
                            # v7.3: ç©ºç™½åˆ—åã€æ•°å­—ã®ã¿ã®åˆ—åã€Unnamedåˆ—ã‚’é™¤å¤–
                            valid_cols = [col for col in df.columns
                                          if col and str(col).strip()
                                          and not str(col).strip().isdigit()
                                          and not str(col).startswith('Unnamed')]
                            df = df[valid_cols]
                            st.dataframe(df, use_container_width=True, hide_index=True)
                    else:
                        df = pd.DataFrame(year_data)
                        st.dataframe(df, use_container_width=True, hide_index=True)
        else:
            st.info("è©•ä¾¡é …ç›®åˆ¥ãƒ‡ãƒ¼ã‚¿ã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    with tab5:
        st.header("ğŸ·ï¸ éƒ¨é–€åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆçµŒå¹´ï¼‰")

        # éƒ¨é–€åˆ¥1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if dept_most_wins:
            st.subheader("ğŸ† éƒ¨é–€åˆ¥ 1ä½ç²å¾—å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            # æœ€æ–°å¹´åº¦ã‚’å–å¾—
            all_years = set()
            for wins_list in dept_most_wins.values():
                for r in wins_list:
                    all_years.update(r.get("years", []))
            latest_year = max(all_years, key=_year_sort_key) if all_years else None

            dept_wins_data = []
            for dept_name, wins_list in dept_most_wins.items():
                for r in wins_list[:3]:  # å„éƒ¨é–€ä¸Šä½3ç¤¾
                    if r["wins"] > 0:
                        # ç¶™ç¶šä¸­ãƒ•ãƒ©ã‚°: æœ€æ–°å¹´åº¦ã‚‚1ä½ãªã‚‰âœ…
                        is_current = latest_year in r.get("years", []) if latest_year else False
                        dept_wins_data.append({
                            "éƒ¨é–€": dept_name,
                            "ä¼æ¥­å": r["company"],
                            "1ä½å›æ•°": r['wins'],  # ã‚½ãƒ¼ãƒˆç”¨ã«æ•°å€¤ã§ä¿æŒ
                            "ç²å¾—ç‡": f"{r['wins']/r['total_years']*100:.1f}%" if r['total_years'] > 0 else "0.0%",
                            "ç¶™ç¶šä¸­": "âœ…" if is_current else "",
                            "ç²å¾—å¹´": ", ".join(map(str, r["years"]))
                        })
            if dept_wins_data:
                # 1ä½å›æ•°ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆ
                dept_wins_data.sort(key=lambda x: -x["1ä½å›æ•°"])
                # è¡¨ç¤ºç”¨ã«å›æ•°ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                for d in dept_wins_data:
                    d["1ä½å›æ•°"] = f"{d['1ä½å›æ•°']}å›"
                st.dataframe(pd.DataFrame(dept_wins_data), use_container_width=True, hide_index=True)
            st.divider()

        # ãƒˆãƒƒãƒ—ã«éƒ¨é–€åˆ¥ã®é€£ç¶š1ä½è¨˜éŒ²
        dept_trends = historical_data.get("dept_trends", {})
        if dept_trends:
            st.subheader("ğŸ·ï¸ éƒ¨é–€åˆ¥ é€£ç¶š1ä½è¨˜éŒ²ï¼ˆä¸Šä½10ä»¶ï¼‰")
            dept_records = []
            for dept_name, data in dept_trends.items():
                for streak in data.get("consecutive_wins", []):
                    # è¤‡æ•°å›1ä½ã‚’ç²å¾—ã—ãŸã‚‚ã®ã®ã¿å¯¾è±¡ï¼ˆ1å¹´ã ã‘ã®å—è³ã¯é™¤å¤–ï¼‰
                    if streak.get("years", 0) >= 2:
                        dept_records.append({
                            "éƒ¨é–€": dept_name,
                            "ä¼æ¥­å": streak["company"],
                            "é€£ç¶šå¹´æ•°": f"{streak['years']}å¹´",
                            "æœŸé–“": f"{streak['start']}ã€œ{streak['end']}",
                            "ç¶™ç¶šä¸­": "âœ…" if streak.get("is_current") else ""
                        })
            if dept_records:
                dept_records.sort(key=lambda x: -int(x["é€£ç¶šå¹´æ•°"].replace("å¹´", "")))
                st.dataframe(pd.DataFrame(dept_records[:10]), use_container_width=True, hide_index=True)
            st.divider()

        # v7.3: ã€Œéƒ¨é–€åˆ¥ å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ï¼ˆTOP10ä¼æ¥­ï¼‰ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤

        if dept_data:
            for dept_name, year_data in dept_data.items():
                # æœ€æ–°åç§°ã‚’å–å¾—ï¼ˆåç§°å¤‰æ›´æƒ…å ±ãŒã‚ã‚Œã°ä½¿ç”¨ï¼‰
                display_name = dept_name
                name_change_info = dept_name_changes.get(dept_name)
                if name_change_info and name_change_info.get("latest_name"):
                    display_name = name_change_info["latest_name"]

                with st.expander(f"ğŸ“Œ {display_name}", expanded=False):
                    # v6.1: ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå¤‰æ›´ - åç§°å¤‰æ›´ â†’ 1ä½ã®æ¨ç§» â†’ çµŒå¹´æ¨ç§» â†’ å¹´æ•°/URL ã®é †ï¼ˆè©•ä¾¡é …ç›®åˆ¥ã¨åŒã˜ï¼‰

                    # 1. åç§°å¤‰æ›´ãŒã‚ã‚Œã°æ³¨è¨˜ã‚’è¡¨ç¤º
                    if name_change_info and name_change_info.get("changes"):
                        for change in name_change_info["changes"]:
                            st.info(f"ğŸ“ **åç§°å¤‰æ›´**: {change['change_year']}å¹´ã‚ˆã‚Šã€Œ{change['from_name']}ã€â†’ã€Œ{change['to_name']}ã€ã«å¤‰æ›´")

                    if isinstance(year_data, dict) and len(year_data) > 1:
                        # 2. 1ä½ã®æ¨ç§»ï¼ˆåç§°å¤‰æ›´ã®ç›´å¾Œã«é…ç½®ï¼‰
                        st.markdown("**ğŸ“ˆ 1ä½ã®æ¨ç§»**")
                        history = []
                        for year in sorted(year_data.keys(), key=_year_sort_key, reverse=True):
                            year_list = year_data.get(year)
                            if year_list and isinstance(year_list, list) and len(year_list) > 0:
                                top = year_list[0]
                                if top and isinstance(top, dict):
                                    history.append({
                                        "å¹´åº¦": year,
                                        "1ä½": top.get("company", "-"),
                                        "å¾—ç‚¹": top.get("score", "-")
                                    })
                        if history:
                            st.dataframe(pd.DataFrame(history), use_container_width=True)

                        # 3. çµŒå¹´å¤‰åŒ–ã®æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ï¼ˆTOP10ä¼æ¥­ã®å¾—ç‚¹æ¨ç§»ï¼‰
                        st.markdown("**ğŸ“Š å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ï¼ˆTOP10ä¼æ¥­ï¼‰**")
                        # æœ€æ–°å¹´åº¦ã®TOP10ä¼æ¥­ã‚’å–å¾—
                        latest_yr = max(year_data.keys(), key=_year_sort_key)
                        latest_top10 = sorted(year_data[latest_yr], key=lambda x: x.get("score") or 0, reverse=True)[:10]
                        top10_companies = [d.get("company") for d in latest_top10 if d.get("company")]

                        line_data = []
                        for yr in sorted(year_data.keys(), key=_year_sort_key):
                            for item in year_data[yr]:
                                company = item.get("company")
                                score = item.get("score")
                                if company in top10_companies and score is not None:
                                    line_data.append({
                                        "å¹´åº¦": str(yr),
                                        "å¾—ç‚¹": score,
                                        "ä¼æ¥­å": company[:15]
                                    })
                        if line_data and len(line_data) > 1:
                            import altair as alt
                            line_df = pd.DataFrame(line_data)
                            # å‹•çš„Yè»¸ç¯„å›²
                            all_scores = [d["å¾—ç‚¹"] for d in line_data]
                            y_min = max(0, min(all_scores) - 3)
                            y_max = max(all_scores) + 3
                            chart = alt.Chart(line_df).mark_line(point=True).encode(
                                x=alt.X('å¹´åº¦:O', title='å¹´åº¦'),
                                y=alt.Y('å¾—ç‚¹:Q', title='å¾—ç‚¹', scale=alt.Scale(domain=[y_min, y_max])),
                                color=alt.Color('ä¼æ¥­å:N', title='ä¼æ¥­å'),
                                tooltip=['å¹´åº¦', 'ä¼æ¥­å', 'å¾—ç‚¹']
                            ).properties(height=300, title=f"{dept_name} å¾—ç‚¹ã®çµŒå¹´æ¨ç§»ï¼ˆTOP10ä¼æ¥­ï¼‰")
                            st.altair_chart(chart, use_container_width=True)

                        st.divider()

                        # 4. å„å¹´åº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆå¹´æ•°/URLï¼‰
                        for year in sorted(year_data.keys(), key=_year_sort_key, reverse=True):
                            # è©²å½“å¹´åº¦ã®URLã‚’å–å¾—
                            year_url = None
                            if used_urls:
                                for url_item in used_urls.get("departments", []):
                                    search_name = f"{dept_name}({year}å¹´)"
                                    if url_item.get("name") == search_name and url_item.get("status") == "success":
                                        year_url = url_item.get("url", "")
                                        break
                            # å¹´åº¦ã®æ¨ªã«URLè¡¨ç¤º
                            if year_url:
                                st.markdown(f"**{year}å¹´** ğŸ”— {year_url}")
                            else:
                                st.markdown(f"**{year}å¹´**")
                            df = pd.DataFrame(year_data[year])
                            # v7.3: ç©ºç™½åˆ—åã€æ•°å­—ã®ã¿ã®åˆ—åã€Unnamedåˆ—ã‚’é™¤å¤–
                            valid_cols = [col for col in df.columns
                                          if col and str(col).strip()
                                          and not str(col).strip().isdigit()
                                          and not str(col).startswith('Unnamed')]
                            df = df[valid_cols]
                            st.dataframe(df, use_container_width=True, hide_index=True)

                    elif isinstance(year_data, dict):
                        # 1å¹´åˆ†ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿
                        for year in sorted(year_data.keys(), key=_year_sort_key, reverse=True):
                            year_url = None
                            if used_urls:
                                for url_item in used_urls.get("departments", []):
                                    search_name = f"{dept_name}({year}å¹´)"
                                    if url_item.get("name") == search_name and url_item.get("status") == "success":
                                        year_url = url_item.get("url", "")
                                        break
                            if year_url:
                                st.markdown(f"**{year}å¹´** ğŸ”— {year_url}")
                            else:
                                st.markdown(f"**{year}å¹´**")
                            df = pd.DataFrame(year_data[year])
                            # v7.3: ç©ºç™½åˆ—åã€æ•°å­—ã®ã¿ã®åˆ—åã€Unnamedåˆ—ã‚’é™¤å¤–
                            valid_cols = [col for col in df.columns
                                          if col and str(col).strip()
                                          and not str(col).strip().isdigit()
                                          and not str(col).startswith('Unnamed')]
                            df = df[valid_cols]
                            st.dataframe(df, use_container_width=True, hide_index=True)
        else:
            st.info("éƒ¨é–€åˆ¥ãƒ‡ãƒ¼ã‚¿ã¯å­˜åœ¨ã—ãªã„ã‹å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    with tab6:
        # ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ä½œæˆã‚¿ãƒ– (v8.0è¿½åŠ )
        if RELEASE_FEATURES_AVAILABLE:
            render_release_tab(
                ranking_name=ranking_name,
                overall_data=overall_data,
                item_data=item_data,
                dept_data=dept_data,
                historical_data=historical_data
            )
        else:
            st.warning("ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹æ©Ÿèƒ½ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.info("release_tab.py, validator.py, release_generator.py, company_master.py ãŒå¿…è¦ã§ã™")

    with tab7:
        st.header("ğŸ“ å‚è€ƒè³‡æ–™ï¼ˆä½¿ç”¨ã—ãŸURLï¼‰")

        if used_urls:
            # ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°URL
            st.subheader("ğŸ“Š ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            overall_urls = used_urls.get("overall", [])
            if overall_urls:
                url_df = pd.DataFrame([
                    {
                        "å¹´åº¦": item.get("year", ""),
                        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "âœ… æˆåŠŸ" if item.get("status") == "success" else "âŒ å¤±æ•—",
                        "URL": item.get("url", "")
                    }
                    for item in overall_urls
                ])
                # URLã‚’ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒªãƒ³ã‚¯ã¨ã—ã¦è¡¨ç¤º
                st.dataframe(
                    url_df,
                    column_config={
                        "URL": st.column_config.LinkColumn("URL", display_text="ğŸ”— ãƒªãƒ³ã‚¯ã‚’é–‹ã")
                    },
                    use_container_width=True,
                    hide_index=True
                )
            else:
                st.info("ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®URLãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            st.divider()

            # è©•ä¾¡é …ç›®åˆ¥URL
            st.subheader("ğŸ“‹ è©•ä¾¡é …ç›®åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            item_urls = used_urls.get("items", [])
            if item_urls:
                url_df = pd.DataFrame([
                    {
                        "é …ç›®å": item.get("name", ""),
                        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "âœ… æˆåŠŸ" if item.get("status") == "success" else "âŒ å¤±æ•—",
                        "URL": item.get("url", "")
                    }
                    for item in item_urls
                ])
                st.dataframe(
                    url_df,
                    column_config={
                        "URL": st.column_config.LinkColumn("URL", display_text="ğŸ”— ãƒªãƒ³ã‚¯ã‚’é–‹ã")
                    },
                    use_container_width=True,
                    hide_index=True
                )
            else:
                st.info("è©•ä¾¡é …ç›®åˆ¥ã®URLãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            st.divider()

            # éƒ¨é–€åˆ¥URL
            st.subheader("ğŸ·ï¸ éƒ¨é–€åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            dept_urls = used_urls.get("departments", [])
            if dept_urls:
                url_df = pd.DataFrame([
                    {
                        "éƒ¨é–€å": item.get("name", ""),
                        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "âœ… æˆåŠŸ" if item.get("status") == "success" else "âŒ å¤±æ•—",
                        "URL": item.get("url", "")
                    }
                    for item in dept_urls
                ])
                st.dataframe(
                    url_df,
                    column_config={
                        "URL": st.column_config.LinkColumn("URL", display_text="ğŸ”— ãƒªãƒ³ã‚¯ã‚’é–‹ã")
                    },
                    use_container_width=True,
                    hide_index=True
                )
            else:
                st.info("éƒ¨é–€åˆ¥ã®URLãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            if uploaded_years and not scraped_years:
                st.info("ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ãŸãŸã‚ã€å‚è€ƒURLã¯ã‚ã‚Šã¾ã›ã‚“")
            else:
                st.info("å‚è€ƒè³‡æ–™ï¼ˆURLæƒ…å ±ï¼‰ãŒã‚ã‚Šã¾ã›ã‚“")

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
        st.divider()
        st.markdown("**ğŸ“Œ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: [ã‚ªãƒªã‚³ãƒ³é¡§å®¢æº€è¶³åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°](https://life.oricon.co.jp/)")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.sidebar.divider()
st.sidebar.markdown("---")
st.sidebar.markdown("ğŸ“Œ **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: life.oricon.co.jp")
st.sidebar.markdown(f"ğŸ”§ **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: {__version__}")

